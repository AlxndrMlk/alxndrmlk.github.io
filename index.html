<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>alxndr.io - Aleksander Molak</title>
        <meta content="width=device-width, initial-scale=1.0" name="viewport">
        <meta content="
        aleksander molak, 
        machine learning, 
        artificial intelligence, 
        ml, 
        ai, 
        natural language processing, 
        nlp,
        probabilistic machine learning,
        bayesian,
        statistics,
        causality,
        inference,
        business,
        consulting
        " name="keywords">
        <meta content="Aleksander Molak's personal site. ML, AI, Machine Learning, NLP, Bayesian methods" name="description">

        <!-- Favicon -->
        <link href="img/favicon.ico" rel="icon">

        <!-- Google Font -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@500&display=swap" rel="stylesheet"> 

        <!-- CSS Libraries -->
        <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.10.0/css/all.min.css" rel="stylesheet">
        <link href="lib/animate/animate.min.css" rel="stylesheet">
        <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">
        <link href="lib/lightbox/css/lightbox.min.css" rel="stylesheet">

        <!-- Template Stylesheet -->
        <link href="css/style.css" rel="stylesheet">


        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-MNGYMD6CC6"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-MNGYMD6CC6');
        </script>
    </head>

    <body data-spy="scroll" data-target=".navbar" data-offset="51">
        <!-- Nav Bar Start -->
        <div class="navbar navbar-expand-lg bg-light navbar-light">
            <div class="container-fluid">
                <a href="index.html" class="navbar-brand">alxndr<span class="navbar-brand-dot">.</span>io</a>
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbarCollapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <div class="collapse navbar-collapse justify-content-between" id="navbarCollapse">
                    <div class="navbar-nav ml-auto">
                        <a href="#home" class="nav-item nav-link active">Home</a>
                        <a href="#about" class="nav-item nav-link">About</a>
                        <a href="#service" class="nav-item nav-link">Talks & Workshops</a>
                        <!-- <a href="#experience" class="nav-item nav-link">Experience</a> -->
                        <!-- <a href="#portfolio" class="nav-item nav-link"></a> -->
                        <!-- <a href="#price" class="nav-item nav-link">Price</a> -->
                        <!-- <a href="#review" class="nav-item nav-link">Review</a> -->
                        <!-- <a href="#team" class="nav-item nav-link">Team</a> -->
                        <a href="#blogblog" class="nav-item nav-link">Blog</a>
                        <a href="#blog" class="nav-item nav-link">Sunday AI Papers</a>
                        <a href="#contact" class="nav-item nav-link">Contact</a>
                    </div>
                </div>
            </div>
        </div>
        <!-- Nav Bar End -->


        <!-- Hero Start -->
        <div class="hero" id="home">
            <div class="container-fluid">
                <div class="row align-items-center">
                    <div class="col-sm-12 col-md-6">
                        <div class="hero-content">
                            <div class="hero-text add-margin-top">
                                <!-- <p>××œ×›×¡× ×“×¨ ××•×œ×§</p> -->
                                <h1>Alexander Molak</h1>
                                <h2></h2>
                                <div class="typed-text">Machine Learning, NLP, Causal Inference, Probabilistic Modeling, Psychology, Business</div>
                            </div>
                            <div class="hero-btn">
                                <a class="btn add-margin-bottom" href="https://www.linkedin.com/in/aleksandermolak/"><i class="fab fa-linkedin-in"> </i> LinkedIn</a>
                                <!-- <a class="btn add-margin-bottom" href="https://github.com/AlxndrMlk"><i class="fab fa-github"></i> GitHub</a> -->
                                <a class="btn add-margin-bottom" href="https://aleksander-molak.medium.com/"><i class="fab fa-medium"></i> Medium</a>
                            </div>
                        </div>
                    </div>
                    <div class="col-sm-12 col-md-6 d-none d-md-block">
                        <div class="hero-image">
                            <img src="img/hero_sm.png" alt="Hero Image">
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Hero End -->


        <!-- About Start -->
        <div class="about wow fadeInUp" data-wow-delay="0.1s" id="about">
            <div class="container-fluid">
                <div class="row align-items-center">
                    <div class="col-lg-6">
                        <div class="about-img">
                            <img src="img/about_sm.jpg" alt="Image">
                        </div>
                    </div>
                    <div class="col-lg-6">
                        <div class="about-content">
                            <div class="section-header text-left">
                                <p>About Me</p>
                                <h2>Crossing the boundaries</h2>
                            </div>
                            <div class="about-text">
                                <p>
                                    I am an Innovation Lead and Machine Learning Researcher currently having a break from regular job.
                                </p>

                                <p>
                                    I am specialized in natural language processing (NLP), causal inference and probabilistic modeling. 
                                </p>

                                <p>
                                    My academic background is in philosophy of language and experimental psychology.
                                    Before starting my career in data science, I used to work as a music producer and mixing engineer. 
                                </p>
                                <p>
                                    Every week on Sunday evening I write a micro-review of a recent paper in machine learning as a part of my <a href="https://www.linkedin.com/in/aleksandermolak/"> LinkedIn</a> project 
                                    <a href="https://www.linkedin.com/feed/hashtag/?keywords=sundayaipapers"><b>Sunday AI Papers</b></a>.
                                </p>
                            </div>
                            <a class="btn" href="#blog">Learn more</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- About End -->
        
        
        <!-- Service Start -->
        <div class="service" id="service">
            <div class="container">
                <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
                    <p>Talks & Workshops</p>
                    <h2>Sharing knowledge</h2>
                </div>

                <p>
                    I am extremely grateful to all the people who shared their knowledge and experience with me. 
                    It's very important to me to give back what I got from others. That's why I love to <b>share</b> my knowledge and experience with others.
                </p>
                <p>
                    Interested in NLP, causal or probabilistic modeling? <b>Join me</b> on one of the upcoming events!<br>
                </p>
                <p>
                    -
                </p>

                <div>
                    <h4>
                        Upcoming events
                        <p></p>
                    </h4>
                </div>

                <div class="row">

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="" title="TBA"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Causal Inference in Python</h3>
                                <p>
                                    Ingolstadt Innovation Conference (TBC)
                                </p>
                            </div>
                        </div>
                    </div>

                </div>

                <div>
                    <h4>
                        Selected past events
                        <p></p>
                    </h4>
                </div>

                <div class="row">

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://2022.pycon.de/" title="Register"><i class="fas fa-code"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Practical graph neural networks in Python with TensorFlow and Spektral (Workshop)</h3>
                                <p>
                                    PyConDE & PyData Berlin 2022 (Apr 13, 2022)
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://www.meetup.com/PyData-Hamburg/events/284616434/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Causality: An Introduction</h3>
                                <p>
                                    PyData Hamburg (Mar 29, 2022)
                                </p>
                                <p>
                                    <a href="https://www.youtube.com/watch?v=kyRUDTexwGM"><i class="fab fa-youtube" style="color: red;"></i></a>
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://ghostday.pl/#agenda" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Causal Disovery in Python</h3>
                                <p>
                                    GHOST Day Applied ML Conf 2022 (Mar 24, 2022)
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://dssconf.pl/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Causal Inference in Python: An Introduction</h3>
                                <p>
                                    Data Science Summit (Dec 3, 2021)
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://www.meetup.com/PyData-Tel-Aviv/events/281354553/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>
                                    What should I buy next? How to leverage word embeddings to build an efficient recommender system
                                </h3>
                                    
                                <p>
                                    PyData Tel Aviv 2021 (Nov 10, 2021)
                                </p>
                                <p>
                                    <a href="https://www.youtube.com/watch?v=_IZUmV28JpA"><i class="fab fa-youtube" style="color: red;"></i></a>
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://pydata.org/global2021/schedule/presentation/13/modeling-aleatoric-and-epistemic-uncertainty-using-tensorflow-and-tensorflow-probability/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Modeling aleatoric and epistemic uncertainty using Tensorflow and Tensorflow Probability</h3>
                                <p>
                                    PyData Global 2021 (Oct 30, 2021)
                                </p>
                                <p>
                                    <a href="https://www.youtube.com/watch?v=KJxmC5GCWe4"><i class="fab fa-youtube" style="color: red;"></i></a>
                                    
                                </p>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.2s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://www.nlpday.pl/" title="Register"><i class="fas fa-code"></i></a>
                            </div>
                            
                            <div class="service-text">
                                <h3>
                                    Uncertainty? Hands-on Bayesian neural networks with Tensorflow and Tensorflow Probability (Workshop)
                                </h3>
                                <p>
                                    NLP & AI Day 2021 (Oct 26, 2021)
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                    <!-- <div>
                        <h4>
                            Archive
                            <p>
                            </p>
                        </h4>
                    </div>

                <div>
                    <ul>

                        <li>
                            <b>What should I buy next? How to leverage word embeddings to build an efficient recommender system.</b>
                            <br>
                            <a href="https://ml.dssconf.pl/">Data Science Summit ML Edition 2021</a>
                            <br><br>
                        </li>

                        <li>
                            <b>Attention! How to efficiently mask sensitive information in unstructured text data?</b>
                            <br>
                            <a href="https://ml.dssconf.pl/">Data Science Summit ML Edition 2021</a>
                            <br><br>
                        </li>

                        <li>
                            <b>Can I do it for you? A case study of the intelligent data imputation system in Master Data Management</b>
                            <br>
                            <a href="https://www.stibosystems.com/connect2020-welcome/">Stibo Global Connect 2020</a>
                            <br><br>                            
                        </li>

                    </ul>

                </div> -->
            </div>
        </div>
        <!-- Service End -->


        <!-- Banner Start -->
        <div class="banner wow zoomIn" data-wow-delay="0.1s">
            <div class="container">
                <div class="section-header text-center">
                    <!-- <p>Blogging</p> -->
                    <h2>Blogging</h2>
                </div>
                <div class="container banner-text">
                    <a class="btn" href="https://www.linkedin.com/in/aleksandermolak/"><i class="fab fa-linkedin-in"></i> LinkedIn</a>
                    <a class="btn" href="https://aleksander-molak.medium.com/"><i class="fab fa-medium"></i> Medium</a>
                </div>
            </div>
        </div>
        <!-- Banner End -->


        <!-- Medium Blog Start -->
        <div class="blog" id="blogblog">
            <div class="container">
                <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
                    <p>Medium Blog</p>
                    <h2>Articles</h2>
                </div>
                <div class="row">

                    <!-- Posts start here L -> R -->

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".15s">
                            <div class="blog-img">
                                <img src="img/medium-books-Q1-2022.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Three amazing data science books to read in 2023 (if you wonâ€™t manage in 2022)</h2>
                                <p>
                                    â€¦and you can read them for free if you want! â¤ï¸
                                </p>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Probabilistic modeling, graph neural networks</p>
                                    <p><i class="far fa-calendar-alt"></i>04-Feb-2022</p>
                                </div>

                                <p>
                                    Kevin Murphy's <a href="https://www.amazon.com/Probabilistic-Machine-Learning-Introduction-Computation/dp/0262046822/ref=sr_1_fkmr1_1?crid=26OWT9WS9ZNIK&amp;keywords=probabilistic+modeling+an+introduction+murphy&amp;qid=1647714837&amp;sprefix=probabilistic+modeling+an+introduction+murphy%252Caps%252C223&amp;sr=8-1-fkmr1&_encoding=UTF8&tag=alxndrmlk0a-20&linkCode=ur2&linkId=d77722e16e8e4f7aef2ab2e20ccb653b&camp=1789&creative=9325">Probabilistic Machine Learning: An Introduction</a>,
                                    <a href="https://www.amazon.com/Bayesian-Modeling-Computation-Chapman-Statistical/dp/036789436X/ref=sr_1_1?crid=20SPWJZNJK0K1&amp;keywords=Bayesian+Modeling+and+Computation+in+Python&amp;qid=1647715035&amp;sprefix=bayesian+modeling+and+computation+in+python%252Caps%252C220&amp;sr=8-1&_encoding=UTF8&tag=alxndrmlk0a-20&linkCode=ur2&linkId=942ad4bca850888e3fd3ed1e069be014&camp=1789&creative=9325">Bayesian Modeling and Computation in Python</a> by Osvaldo Martin and colleagues and 
                                    <a href="https://www.amazon.com/Deep-Learning-Graphs-Yao-Ma/dp/1108831745/ref=sr_1_3?crid=123RUAZEP2CJ6&amp;keywords=3.+Deep+Learning+on+Graphs&amp;qid=1647715188&amp;sprefix=3.+deep+learning+on+graphs%252Caps%252C503&amp;sr=8-3&_encoding=UTF8&tag=alxndrmlk0a-20&linkCode=ur2&linkId=bd8e2dd6260b1fbaa402868fc398cf35&camp=1789&creative=9325">Deep Learning on Graphs</a>. What are they about and what to expect inside? 
                                </p>
                                <a class="btn" href="https://aleksander-molak.medium.com/three-amazing-data-science-books-to-read-in-2023-if-you-wont-manage-in-2022-f35827cbbb1d">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".15s">
                            <div class="blog-img">
                                <img src="img/medium-tfp-04.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Modeling uncertainty in neural networks with TensorFlow Probability - Part 4</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Probabilistic modeling, Bayesian neural networks, TensorFlow Probability</p>
                                    <p><i class="far fa-calendar-alt"></i>26-Nov-2021</p>
                                </div>
                                <p>
                                    Part 4: Going fully probabilistic
                                </p>
                                <a class="btn" href="https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-391b29538a7a">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".15s">
                            <div class="blog-img">
                                <img src="img/medium-tfp-03.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Modeling uncertainty in neural networks with TensorFlow Probability - Part 3</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Probabilistic modeling, Bayesian neural networks, TensorFlow Probability</p>
                                    <p><i class="far fa-calendar-alt"></i>19-Nov-2021</p>
                                </div>
                                <p>
                                    Part 3: Epistemic uncertainty
                                </p>
                                <a class="btn" href="https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-d519a4426e9c">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

        
                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".15s">
                            <div class="blog-img">
                                <img src="img/medium-tfp-02.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Modeling uncertainty in neural networks with TensorFlow Probability - Part 2</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Probabilistic modeling, Bayesian neural networks, TensorFlow Probability</p>
                                    <p><i class="far fa-calendar-alt"></i>12-Nov-2021</p>
                                </div>
                                <p>
                                    Part 2: Aleatoric uncertainty
                                </p>
                                <a class="btn" href="https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-a706c2274d12">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/medium-tfp-01.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Modeling uncertainty in neural networks with TensorFlow Probability - Part 1</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Probabilistic modeling, Bayesian neural networks, TensorFlow Probability</p>
                                    <p><i class="far fa-calendar-alt"></i>03-Nov-2021</p>
                                </div>
                                <p>
                                    Part 1: An Introduction
                                </p>
                                <a class="btn" href="https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-part-1-an-introduction-2bb564c67d6">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                </div>
            </div>
        </div>

        
        
        <!-- Banner Start -->
        <div class="banner wow zoomIn" data-wow-delay="0.1s">
            <div class="container">
                <div class="section-header text-center">
                    <p>Let's stay in touch!</p>
                    <h2>Let's connect!</h2>
                </div>
                <div class="container banner-text">
                    <p>
                        I'd love to invite you to my LinkedIn and Medium networks. Join the community around Sunday AI Papers and connect to thousands of machine learning researchers and practitioners!
                    </p>
                    <a class="btn" href="https://www.linkedin.com/in/aleksandermolak/"><i class="fab fa-linkedin-in"></i></a>
                    <a class="btn" href="https://aleksander-molak.medium.com/"><i class="fab fa-medium"></i></a>
                    <a class="btn" href="https://github.com/AlxndrMlk"><i class="fab fa-github"></i></a>
                </div>
            </div>
        </div>
        <!-- Banner End -->


        <!-- Blog Start -->
        <div class="blog" id="blog">
            <div class="container">
                <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
                    <p>Sunday AI Papers</p>
                    <h2>Latest posts</h2>
                </div>
                <div class="row">

                    <!-- Posts start here L -> R -->

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".1s">
                            <div class="blog-img">
                                <img src="img/blog-26.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>âœ¨ Are ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ğ˜€ ğ—¿ğ—¼ğ—¯ğ˜‚ğ˜€ğ˜ to ğ˜€ğ—½ğ˜‚ğ—¿ğ—¶ğ—¼ğ˜‚ğ˜€ ğ—°ğ—¼ğ—¿ğ—¿ğ—²ğ—¹ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Transformers, spurious correlations, invariance</p>
                                    <p><i class="far fa-calendar-alt"></i>20-Mar-2022</p>
                                </div>
                                <p>
                                    Last Thursday researchers from University of Wisconsin-Madison released a brand new paper 
                                    analyzing the robustness of ViT model to spurious correlations.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6911383218692718592-AwVK?utm_source=linkedin_share&utm_medium=member_desktop_web">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".1s">
                            <div class="blog-img">
                                <img src="img/blog-25.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2> âœ¨ Scaling ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ğ˜€ to thousands of layers? ğŸ˜¯</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, scaling</p>
                                    <p><i class="far fa-calendar-alt"></i>06-Mar-2022</p>
                                </div>
                                <p>
                                    Last Tuesday researchers form Microsoft Research released a new paper introducing 
                                    a new method that allows to build ğ—²ğ˜…ğ˜ğ—¿ğ—²ğ—ºğ—²ğ—¹ğ˜† ğ—±ğ—²ğ—²ğ—½ Transformer models.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_stopwar-sundayaipapers-machinelearning-activity-6906326045860184065-eBeh">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".1s">
                            <div class="blog-img">
                                <img src="img/blog-24.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2> âœ¨ Making ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ğ˜€ 4-ğŸ®ğŸ­ğ˜… ğ—³ğ—®ğ˜€ğ˜ğ—²ğ—¿, but ğ˜„ğ—¶ğ˜ğ—µğ—¼ğ˜‚ğ˜ ğ—¾ğ˜‚ğ—®ğ—¹ğ—¶ğ˜ğ˜† ğ—±ğ—¿ğ—¼ğ—½? Check this! ğŸ’¥</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, attention</p>
                                    <p><i class="far fa-calendar-alt"></i>27-Feb-2022</p>
                                </div>
                                <p>
                                    Last Monday researchers form Cornell University and Google Brain released 
                                    a new paper presenting ğ—™ğ—Ÿğ—”ğ—¦ğ—› - a novel efficient modification of Transformer 
                                    architecture.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6903793258288549889-C_IR">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".1s">
                            <div class="blog-img">
                                <img src="img/blog-23.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2> Have you ever thought about ğ—°ğ—µğ—®ğ—»ğ—´ğ—¶ğ—»ğ—´ your ğ—±ğ—®ğ˜ğ—® ğ—±ğ˜‚ğ—¿ğ—¶ğ—»ğ—´ the ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´? ğŸ˜¯</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Deep Learning, training, schedules</p>
                                    <p><i class="far fa-calendar-alt"></i>20-Feb-2022</p>
                                </div>
                                <p>
                                    On Thursday, Leslie N. Smith of U.S. Naval Research Laboratory released his new paper on 
                                    general cyclical training. Sound familiar? Maybe, but don't get misled!
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6901232356880187392-SxXx">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".1s">
                            <div class="blog-img">
                                <img src="img/blog-22.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2> ğ—˜ğ—»ğ—±-ğ˜ğ—¼-ğ—²ğ—»ğ—± Bayesian ğ—°ğ—®ğ˜‚ğ˜€ğ—®ğ—¹ ğ—¶ğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² with theoretical guarantees? Yes! ğŸ¤¯</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Causality, Bayesian methods</p>
                                    <p><i class="far fa-calendar-alt"></i>13-Feb-2022</p>
                                </div>
                                <p>
                                    On the first Friday of February, researchers from Microsoft Research, University of Cambridge, 
                                    University of Massachusetts Amherst and G-Research released a paper describing a novel method for 
                                    end-to-end causal process.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6898723493614366720-nQ9r">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-21.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Commonsense ğ—°ğ—®ğ˜‚ğ˜€ğ—®ğ—¹ğ—¶ğ˜ğ˜† using ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ğ˜€?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, causality, Transformers</p>
                                    <p><i class="far fa-calendar-alt"></i>06-Feb-2022</p>
                                </div>
                                <p>
                                    Last Monday, researchers from University of Pennsylvania released a paper proposing a new 
                                    framework to perform ğ—°ğ—¼ğ—ºğ—ºğ—¼ğ—»ğ˜€ğ—²ğ—»ğ˜€ğ—² ğ—°ğ—®ğ˜‚ğ˜€ğ—®ğ—¹ğ—¶ğ˜ğ˜† ğ—¿ğ—²ğ—®ğ˜€ğ—¼ğ—»ğ—¶ğ—»ğ—´ (ğ—–ğ—–ğ—¥).
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6896191256461201409-EsUK">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>
                    

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-20.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Can ğ—°ğ—®ğ˜‚ğ˜€ğ—®ğ—¹ğ—¶ğ˜ğ˜† help in making ğ—°ğ—¼ğ—»ğ—±ğ—¶ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ˜ğ—²ğ˜…ğ˜ ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» more ğ˜ƒğ—²ğ—¿ğ˜€ğ—®ğ˜ğ—¶ğ—¹ğ—² and ğ—¹ğ—²ğ˜€ğ˜€ ğ—¯ğ—¶ğ—®ğ˜€ğ—²ğ—±?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, causality, VAE</p>
                                    <p><i class="far fa-calendar-alt"></i>30-Jan-2022</p>
                                </div>
                                <p>
                                    Last Saturday, researchers from UC San Diego and Amazon AI released a paper describing a novel approach 
                                    to conditional text generation that leverages causal inference principles to mitigate the effects of spurious correlations. 
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6893652455415238657-PDcO">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-19.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>More ğ˜€ğ—²ğ—ºğ—®ğ—»ğ˜ğ—¶ğ—°ğ—®ğ—¹ğ—¹ğ˜† ğ˜€ğ—²ğ—»ğ˜€ğ—¶ğ˜ğ—¶ğ˜ƒğ—² contrastive ğ˜€ğ—²ğ—»ğ˜ğ—²ğ—»ğ—°ğ—² ğ—²ğ—ºğ—¯ğ—²ğ—±ğ—±ğ—¶ğ—»ğ—´ğ˜€?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, contrastive learning</p>
                                    <p><i class="far fa-calendar-alt"></i>23-Jan-2022</p>
                                </div>
                                <p>
                                    Last Wednesday, researchers from The University of Hong Kong, National University of Defense Technology and SenseTime å•†æ±¤ç§‘æŠ€ 
                                    released a paper proposing a new contrastive sentence embedding framework called ğ—¦ğ—¡ğ—–ğ—¦ğ—˜.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6891102099477000196-Vp5J">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-18.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ğ˜€ with efficient ğ˜‚ğ—»ğ—°ğ—²ğ—¿ğ˜ğ—®ğ—¶ğ—»ğ˜ğ˜† ğ—²ğ˜€ğ˜ğ—¶ğ—ºğ—®ğ˜ğ—¶ğ—¼ğ—»? ğŸ˜¯ VoilÃ ! ğŸ‰</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, uncertainty, Bayesian, attention</p>
                                    <p><i class="far fa-calendar-alt"></i>09-Jan-2022</p>
                                </div>
                                <p>
                                    On the last Monday of December, researchers from University of Amsterdam and Amazon released a paper 
                                    introducing a novel uncertainty estimation method for Transformers. 
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6886040535644037120-mZt2">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-16.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Is causal ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—² ğ—²ğ—»ğ—°ğ—¼ğ—±ğ—²ğ—± in the ğ—¹ğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—²? Making reinforcement learning agents more robust by asking them to ğ—²ğ˜…ğ—½ğ—¹ğ—®ğ—¶ğ—» their ğ—±ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—»ğ˜€.</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>RL, NLP, causality, generalizability</p>
                                    <p><i class="far fa-calendar-alt"></i>12-Dec-2021</p>
                                </div>
                                <p>
                                    Last Wednesday, researchers from DeepMind released a paper describing 
                                    a novel approach to RL-agent training that makes the agents more robust.
                                    Agents were able to learn more generalizable abstractions thanks to... 
                                    explaining their decisions.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6875915126214598656-ezrh">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-15.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>ğ—šğ—£ğ—¨-ğ—®ğ—°ğ—°ğ—²ğ—¹ğ—²ğ—¿ğ—®ğ˜ğ—²ğ—± gradient-based ğ—°ğ—®ğ˜‚ğ˜€ğ—®ğ—¹ ğ—±ğ—¶ğ˜€ğ—°ğ—¼ğ˜ƒğ—²ğ—¿ğ˜† in Python? ğ—›ğ—²ğ—¿ğ—² ğ˜†ğ—¼ğ˜‚ ğ—´ğ—¼!</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Causality, causal discovery</p>
                                    <p><i class="far fa-calendar-alt"></i>05-Dec-2021</p>
                                </div>
                                <p>
                                    Last Tuesday, researchers from Huawei Noah's Ark Lab and University of Toronto 
                                    released a new causal discovery package and an accompanying paper. 
                                    It brings some really cool features to the table!
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-causality-activity-6873357904792104960--AHC">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-14.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Can one learn a ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—² of a ğ—°ğ—®ğ˜‚ğ˜€ğ—®ğ—¹ ğ—´ğ—¿ğ—®ğ—½ğ—µ with ğ—¹ğ—®ğ˜ğ—²ğ—»ğ˜ ğ˜ƒğ—®ğ—¿ğ—¶ğ—®ğ—¯ğ—¹ğ—²ğ˜€?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Causality, causal discovery</p>
                                    <p><i class="far fa-calendar-alt"></i>28-Nov-2021</p>
                                </div>
                                <p>
                                    Last Sunday, researchers from University of Chicago and Carnegie Mellon University 
                                    released a paper proposing a novel method of discovering a causal graph with latent 
                                    variables, but the problem is hard.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-causality-activity-6870844568338804736-QvWR">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-13.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>ğ——ğ—¶ğ˜€ğ˜ğ—¶ğ—¹ğ—¹ğ—¶ğ—»ğ—´ ğ—•ğ—˜ğ—¥ğ—§ better? Get ğ—³ğ—®ğ˜€ğ˜ğ—²ğ—¿ and more ğ—¿ğ—¼ğ—¯ğ˜‚ğ˜€ğ˜!</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, distillation, compression</p>
                                    <p><i class="far fa-calendar-alt"></i>21-Nov-2021</p>
                                </div>
                                <p>
                                    Last Thursday, researchers from Intel Labs and University of California, Santa Barbara 
                                    proposed a new approach to model distillation. 
                                    The proposed architecture achieves very good trade-off between ğ—¶ğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ˜ğ—¶ğ—ºğ—² ğ—¿ğ—²ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» and ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜†.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-nlp-activity-6868300191968100352-BVP9">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-12.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Contrastive loss ğ—•ğ—˜ğ—¥ğ—§ pre-training for ğ—¶ğ—ºğ—½ğ—¿ğ—¼ğ˜ƒğ—²ğ—± ğ—¿ğ—²ğ—½ğ—¿ğ—²ğ˜€ğ—²ğ—»ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€? Yes, please!</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, contrastive loss, pre-training</p>
                                    <p><i class="far fa-calendar-alt"></i>14-Nov-2021</p>
                                </div>
                                <p>
                                    Last Tuesday, researchers from University of Cambridge, Amazon Web Services (AWS) AI and 
                                    Monash University released a paper introducing a new pre-training approach leveraging 
                                    contrastive loss scheme. 
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-nlp-activity-6865758937896022016-mDLY">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>
                    
                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-11.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Ok, computer, ğ—½ğ—¿ğ—²ğ—½ğ—¿ğ—¼ğ—°ğ—²ğ˜€ğ˜€ my ğ˜€ğ˜ğ—¿ğ—¶ğ—»ğ—´ğ˜€! ğŸ¤–</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, strings, preprocessing</p>
                                    <p><i class="far fa-calendar-alt"></i>07-Nov-2021</p>
                                </div>
                                <p>
                                    Last Thursday researchers from Eindhoven University of Technology released a paper describing a 
                                    framework for automated string preprocessing and encoding. 
                                    The framework leverages probabilistic type inference among other interesting components.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-sundayaipapers-machinelearning-activity-6863146269716598784-v7Hr">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <!-- <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-10.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Is ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—¼ğ—¯ğ˜€ğ—¼ğ—¹ğ—²ğ˜ğ—²? Can we ğ—½ğ—¿ğ—²ğ—±ğ—¶ğ—°ğ˜ the ğ˜„ğ—²ğ—¶ğ—´ğ—µğ˜ğ˜€ instead?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Parameter search, optimization, GNNs</p>
                                    <p><i class="far fa-calendar-alt"></i>31-Oct-2021</p>
                                </div>
                                <p>
                                    Last Monday, researchers from University of Guelph and Facebook AI 
                                    released a paper presenting a novel framework that predicts parameters 
                                    for an arbitrary network**. What's behind?
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ai-activity-6860681839154135040-lnFc">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-9.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2> Can you broaden my perspective? ğ—–ğ—¼ğ—»ğ˜ğ—¿ğ—®ğ˜€ğ˜ğ—¶ğ˜ƒğ—² Document Representation Learning with ğ—šğ—¿ğ—®ğ—½ğ—µ ğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—» ğ—¡ğ—²ğ˜ğ˜„ğ—¼ğ—¿ğ—¸ğ˜€ over ğ—½ğ—¿ğ—²-ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—²ğ—± ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ğ˜€.</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, contrastive learning, GNNs</p>
                                    <p><i class="far fa-calendar-alt"></i>24-Oct-2021</p>
                                </div>
                                <p>
                                    Last Wednesday, researchers from AWS AI Labs released a paper presenting 
                                    a novel method for ğ—±ğ—¼ğ—°ğ˜‚ğ—ºğ—²ğ—»ğ˜ ğ—¿ğ—²ğ—½ğ—¿ğ—²ğ˜€ğ—²ğ—»ğ˜ğ—®ğ˜ğ—¶ğ—¼ğ—». The method uses a large language model, 
                                    a graph attention neural network and a contrastive training strategy. 
                                    Obtained document representations seem to have ğ˜ƒğ—²ğ—¿ğ˜† ğ—´ğ—¼ğ—¼ğ—± ğ—½ğ—¿ğ—¼ğ—½ğ—²ğ—¿ğ˜ğ—¶ğ—²ğ˜€.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6858125702483496960-Tdh9">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-8.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Hey ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ğ˜€, how ğ˜€ğ—²ğ—»ğ˜€ğ—¶ğ˜ğ—¶ğ˜ƒğ—² are you to ğ˜€ğ—½ğ˜‚ğ—¿ğ—¶ğ—¼ğ˜‚ğ˜€ perturbations?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, causality</p>
                                    <p><i class="far fa-calendar-alt"></i>17-Oct-2021</p>
                                </div>
                                <p>
                                    The ties between NLP and Causal Inference grow stronger. 
                                    Last Thursday, a group of researchers from Peking University and 
                                    Normal University of Singapore released a paper analysing four 
                                    NLP architectures' sensitivity to spurious features using Causal Inference tools.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6855600302754713600-Qfq3">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-7.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Data collection, ğ—°ğ—®ğ˜‚ğ˜€ğ—®ğ—¹ğ—¶ğ˜ğ˜† and ğ—¡ğ—Ÿğ—£?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, causality</p>
                                    <p><i class="far fa-calendar-alt"></i>10-Oct-2021</p>
                                </div>
                                <p>
                                    Last Friday researchers from Max Planck Institute for Intelligent Systems, 
                                    ETH ZÃ¼rich, University of Cambridge, UCL and 
                                    Indian Institute of Technology, Kharagpur published a paper 
                                    examining a relationship between data collection process and models 
                                    performance on various NLP tasks. 
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6853075833230286848-sn38">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-6.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Does ğ—•ğ—˜ğ—¥ğ—§ really ğ—±ğ—¼ what ğ˜†ğ—¼ğ˜‚ ğ˜ğ—µğ—¶ğ—»ğ—¸ it ğ—±ğ—¼ğ—²ğ˜€?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, BERT, permutation</p>
                                    <p><i class="far fa-calendar-alt"></i>03-Oct-2021</p>
                                </div>
                                <p>
                                    Last week, we've seen how ğ—½ğ—¼ğ—¼ğ—¿ğ—¹ğ˜† BERT might ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ—¹ğ—¶ğ˜‡ğ—² 
                                    in named entity recognition (ğ—¡ğ—˜ğ—¥) tasks (https://lnkd.in/e-zfBNg9). 
                                    This week we'll look at a paper released on Tuesday, 
                                    where researchers from Sberbank and Higher School of Economics 
                                    in Moscow systematically examine ğ˜€ğ˜†ğ—»ğ˜ğ—®ğ—°ğ˜ğ—¶ğ—° ğ—®ğ—¯ğ—¶ğ—¹ğ—¶ğ˜ğ—¶ğ—²ğ˜€ of the model.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6850491510715146241-TCai">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-5.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>ğ—›ğ—¼ğ˜„ ğ—²ğ—®ğ˜€ğ—¶ğ—¹ğ˜† could you ğ—ºğ—¶ğ˜€ğ—¹ğ—²ğ—®ğ—± ğ—•ğ—˜ğ—¥ğ—§ in recognizing ğ—»ğ—®ğ—ºğ—²ğ—± ğ—²ğ—»ğ˜ğ—¶ğ˜ğ—¶ğ—²ğ˜€ (NER)?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, adversarial attacks</p>
                                    <p><i class="far fa-calendar-alt"></i>26-Sep-2021</p>
                                </div>
                                <p>
                                    Last Thursday researchers from Leiden University released a paper 
                                    examining the influence of ğ—®ğ—±ğ˜ƒğ—²ğ—¿ğ˜€ğ—®ğ—¿ğ—¶ğ—®ğ—¹ ğ—®ğ˜ğ˜ğ—®ğ—°ğ—¸ğ˜€ on ğ—•ğ—˜ğ—¥ğ—§'s capabilities in 
                                    ğ—¡ğ—®ğ—ºğ—²ğ—± ğ—˜ğ—»ğ˜ğ—¶ğ˜ğ˜† ğ—¥ğ—²ğ—°ğ—¼ğ—´ğ—»ğ—¶ğ˜ğ—¶ğ—¼ğ—» (NER).
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6847996331672297473-OhLo">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.3s">
                            <div class="blog-img">
                                <img src="img/blog-4.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Can ğ—¿ğ—®ğ—»ğ—±ğ—¼ğ—ºğ—¹ğ˜†-ğ˜„ğ—²ğ—¶ğ—´ğ—µğ˜ğ—²ğ—± one-layer ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ be as good as a ğ—³ğ˜‚ğ—¹ğ—¹ğ˜† ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—²ğ—± model?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, memory efficiency</p>
                                    <p><i class="far fa-calendar-alt"></i>12-Sep-2021</p>
                                </div>
                                <p>
                                    Last Wednesday researchers from Facebook AI and University of California, Berkeley 
                                    released a paper examining the potential of randomly-weighted one-layer 
                                    Transformer for translation and other NLP tasks.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6842896382714548224-HPG-">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.3s">
                            <div class="blog-img">
                                <img src="img/blog-3.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Who wants an ğ—²ğ—³ğ—³ğ—¶ğ—°ğ—¶ğ—²ğ—»ğ˜ ğ˜ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ for ğ—®ğ—¿ğ—¯ğ—¶ğ˜ğ—¿ğ—®ğ—¿ğ—¶ğ—¹ğ˜† ğ—¹ğ—¼ğ—»ğ—´ ğ˜€ğ—²ğ—¾ğ˜‚ğ—²ğ—»ğ—°ğ—²ğ˜€? </h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, memory efficiency</p>
                                    <p><i class="far fa-calendar-alt"></i>05-Sep-2021</p>
                                </div>
                                <p>
                                    Last Wednesday researchers from University of Lisbon and DeepMind released 
                                    a paper where they proposed an infinite-memory transformer or âˆ-ğ™›ğ™¤ğ™§ğ™¢ğ™šğ™§. 
                                    The model is able to model arbitrarily long contexts.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6840375144599367680-IACv">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.3s">
                            <div class="blog-img">
                                <img src="img/blog-2.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Have you ever heard of ğ˜ƒğ—®ğ—¿ğ—¶ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ—®ğ˜‚ğ˜ğ—¼ğ—²ğ—»ğ—°ğ—¼ğ—±ğ—²ğ—¿ğ˜€ (VAEs) for ğ—¡ğ—Ÿğ—£?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, variational inference</p>
                                    <p><i class="far fa-calendar-alt"></i>29-Aug-2021</p>
                                </div>
                                <p>
                                    Last Monday researchers from Universidad Carlos III de Madrid and Swiss Data Science Institute (ETHZ/EPFL) 
                                    released a paper, proposing a novel and surprising usage of ğ—©ğ—”ğ—˜s within ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ğ˜€ framework. 
                                    ğ—¡ğ—¼ğ—¥ğ—•ğ—˜ğ—¥ğ—§ is a modified ğ—•ğ—˜ğ—¥ğ—§ architecture with a special VAE component.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6837843979887808512-jAUO">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.1s">
                            <div class="blog-img">
                                <img src="img/blog-1.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Dreaming about ğ—¿ğ—¼ğ—¯ğ˜‚ğ˜€ğ˜ ğ—¡ğ—˜ğ—¥ models with ğ—½ğ—®ğ—¿ğ˜ğ—¶ğ—®ğ—¹ğ—¹ğ˜† ğ—¹ğ—®ğ—¯ğ—²ğ—¹ğ—¹ğ—²ğ—± data?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, NER, Transformers</p>
                                    <p><i class="far fa-calendar-alt"></i>22-Aug-2021</p>
                                </div>
                                <p>
                                    Last week researchers from Columbia University in the City of New York and 
                                    Google Research released a paper proposing a new approach towards partially 
                                    supervised named entity recognition (NER).
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6835313599288799232-FY-R">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div> -->




                </div>
            </div>
        </div>
        <!-- Blog End -->


        <!-- Footer Start -->
        <div class="footer wow fadeIn" data-wow-delay="0.3s" id="contact">
            <div class="container-fluid">
                <div class="container">
                    <div class="footer-info">
                        <h2>Let's connect!</h2>
                        <div class="footer-social">
                            <a href="https://www.linkedin.com/in/aleksandermolak"><i class="fab fa-linkedin-in"></i></a>
                            <a href="https://github.com/AlxndrMlk"><i class="fab fa-github"></i></a>
                            <a href="https://twitter.com/AleksanderMolak"><i class="fab fa-twitter"></i></a>
                            <a href="https://www.researchgate.net/profile/Aleksander-Molak"><i class="fab fa-researchgate"></i></a>
                            <a href="https://aleksander-molak.medium.com/"><i class="fab fa-medium"></i></a>
                            <!-- <a href=""><i class="fab fa-youtube"></i></a>
                            <a href=""><i class="fab fa-instagram"></i></a> -->
                        </div>
                    </div>
                </div>
                <div class="container copyright">
                    <p>&copy; <a href="#">alxndr.io 2021</a></p>
                    <p class='small-letters'>Based on a template from <a href="https://htmlcodex.com">HTML Codex</a></p>
                </div>
            </div>
        </div>
        <!-- Footer End -->
        
        
        <!-- Back to top button -->
        <a href="#" class="btn back-to-top"><i class="fa fa-chevron-up"></i></a>
        
        
        <!-- Pre Loader -->
        <div id="loader" class="show">
            <div class="loader"></div>
        </div>

        
        <!-- JavaScript Libraries -->
        <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>
        <script src="lib/easing/easing.min.js"></script>
        <script src="lib/wow/wow.min.js"></script>
        <script src="lib/waypoints/waypoints.min.js"></script>
        <script src="lib/typed/typed.min.js"></script>
        <script src="lib/owlcarousel/owl.carousel.min.js"></script>
        <script src="lib/isotope/isotope.pkgd.min.js"></script>
        <script src="lib/lightbox/js/lightbox.min.js"></script>
        
        <!-- Contact Javascript File -->
        <script src="mail/jqBootstrapValidation.min.js"></script>
        <script src="mail/contact.js"></script>

        <!-- Template Javascript -->
        <script src="js/main.js"></script>
    </body>
</html>
