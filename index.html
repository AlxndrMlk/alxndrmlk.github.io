<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>alxndr.io - Aleksander Molak</title>
        <meta content="width=device-width, initial-scale=1.0" name="viewport">
        <meta content="
        aleksander molak, 
        machine learning, 
        artificial intelligence, 
        ml, 
        ai, 
        natural language processing, 
        nlp,
        probabilistic machine learning,
        bayesian,
        statistics,
        causality,
        inference,
        business,
        consulting
        " name="keywords">
        <meta content="Aleksander Molak's personal site. ML, AI, Machine Learning, NLP, Bayesian methods" name="description">

        <!-- Favicon -->
        <link href="img/favicon.ico" rel="icon">

        <!-- Google Font -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@500&display=swap" rel="stylesheet"> 

        <!-- CSS Libraries -->
        <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.10.0/css/all.min.css" rel="stylesheet">
        <link href="lib/animate/animate.min.css" rel="stylesheet">
        <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">
        <link href="lib/lightbox/css/lightbox.min.css" rel="stylesheet">

        <!-- Template Stylesheet -->
        <link href="css/style.css" rel="stylesheet">


        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-MNGYMD6CC6"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-MNGYMD6CC6');
        </script>
    </head>

    <body data-spy="scroll" data-target=".navbar" data-offset="51">
        <!-- Nav Bar Start -->
        <div class="navbar navbar-expand-lg bg-light navbar-light">
            <div class="container-fluid">
                <a href="index.html" class="navbar-brand">alxndr<span class="navbar-brand-dot">.</span>io</a>
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbarCollapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <div class="collapse navbar-collapse justify-content-between" id="navbarCollapse">
                    <div class="navbar-nav ml-auto">
                        <a href="#home" class="nav-item nav-link active">Home</a>
                        <a href="#about" class="nav-item nav-link">About</a>
                        <a href="#service" class="nav-item nav-link">Talks & Workshops</a>
                        <!-- <a href="#experience" class="nav-item nav-link">Experience</a> -->
                        <!-- <a href="#portfolio" class="nav-item nav-link"></a> -->
                        <!-- <a href="#price" class="nav-item nav-link">Price</a> -->
                        <!-- <a href="#review" class="nav-item nav-link">Review</a> -->
                        <!-- <a href="#team" class="nav-item nav-link">Team</a> -->
                        <a href="#blog" class="nav-item nav-link">Sunday AI Papers</a>
                        <a href="#contact" class="nav-item nav-link">Contact</a>
                    </div>
                </div>
            </div>
        </div>
        <!-- Nav Bar End -->


        <!-- Hero Start -->
        <div class="hero" id="home">
            <div class="container-fluid">
                <div class="row align-items-center">
                    <div class="col-sm-12 col-md-6">
                        <div class="hero-content">
                            <div class="hero-text add-margin-top">
                                <!-- <p>××œ×›×¡× ×“×¨ ××•×œ×§</p> -->
                                <h1>Alexander Molak</h1>
                                <h2></h2>
                                <div class="typed-text">Machine Learning, NLP, Causal Inference, Probabilistic Modeling, Psychology, Business</div>
                            </div>
                            <div class="hero-btn">
                                <a class="btn add-margin-bottom" href="https://www.linkedin.com/in/aleksandermolak/"><i class="fab fa-linkedin-in"> </i> LinkedIn</a>
                                <a class="btn add-margin-bottom" href="https://github.com/AlxndrMlk"><i class="fab fa-github"></i> GitHub</a>
                            </div>
                        </div>
                    </div>
                    <div class="col-sm-12 col-md-6 d-none d-md-block">
                        <div class="hero-image">
                            <img src="img/hero_sm.png" alt="Hero Image">
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Hero End -->


        <!-- About Start -->
        <div class="about wow fadeInUp" data-wow-delay="0.1s" id="about">
            <div class="container-fluid">
                <div class="row align-items-center">
                    <div class="col-lg-6">
                        <div class="about-img">
                            <img src="img/about_sm.jpg" alt="Image">
                        </div>
                    </div>
                    <div class="col-lg-6">
                        <div class="about-content">
                            <div class="section-header text-left">
                                <p>About Me</p>
                                <h2>Crossing the boundaries</h2>
                            </div>
                            <div class="about-text">
                                <p>
                                    I am an Innovation Lead and Machine Learning Researcher at <a href="https://lingarogroup.com/">Lingaro</a>, 
                                    where I build production-grade machine learning systems for Fortune 100 companies.
                                </p>

                                <p>
                                    I am specialized in natural language processing (NLP), causal inference and probabilistic modeling. 
                                </p>

                                <p>
                                    My academic background is in philosophy of language and experimental psychology.
                                    Before starting my career in data science, I used to work as a music producer and mixing engineer. 
                                </p>
                                <p>
                                    Every week on Sunday evening I write a micro-review of a recent paper in machine learning as a part of my <a href="https://www.linkedin.com/in/aleksandermolak/"> LinkedIn</a> project 
                                    <a href="https://www.linkedin.com/feed/hashtag/?keywords=sundayaipapers"><b>Sunday AI Papers</b></a>.
                                </p>
                            </div>
                            <a class="btn" href="#blog">Learn more</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- About End -->
        
        
        <!-- Service Start -->
        <div class="service" id="service">
            <div class="container">
                <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
                    <p>Talks & Workshops</p>
                    <h2>Sharing knowledge</h2>
                </div>

                <p>
                    I am extremely grateful to all the people who shared their knowledge and experience with me. 
                    It's very important to me to give back what I got from others. That's why I love to <b>share</b> my knowledge and experience with others.
                </p>
                <p>
                    Interested in NLP, causal or probabilistic modeling? <b>Join me</b> on one of the upcoming events!<br>
                </p>
                <p>
                    -
                </p>

                <div>
                    <h4>
                        Upcoming events
                        <p></p>
                    </h4>
                </div>

                <div class="row">

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://dssconf.pl/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Causal Inference in Python: An Introduction</h3>
                                <p>
                                    Data Science Summit (Dec 3, 2021)
                                </p>
                            </div>
                        </div>
                    </div>

                </div>

                <div>
                    <h4>
                        Past events
                        <p></p>
                    </h4>
                </div>

                <div class="row">

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://www.meetup.com/PyData-Tel-Aviv/events/281354553/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>
                                    What should I buy next? How to leverage word embeddings to build an efficient recommender system
                                </h3>
                                    
                                <p>
                                    PyData Tel Aviv 2021 (Nov 10, 2021)
                                </p>
                                <p>
                                    <a href="https://www.youtube.com/watch?v=_IZUmV28JpA"><i class="fab fa-youtube" style="color: red;"></i></a>
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://pydata.org/global2021/schedule/presentation/13/modeling-aleatoric-and-epistemic-uncertainty-using-tensorflow-and-tensorflow-probability/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Modeling aleatoric and epistemic uncertainty using Tensorflow and Tensorflow Probability</h3>
                                <p>
                                    PyData Global 2021 (Oct 30, 2021)
                                </p>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.2s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://www.nlpday.pl/" title="Register"><i class="fas fa-code"></i></a>
                            </div>
                            
                            <div class="service-text">
                                <h3>
                                    Uncertainty? Hands-on Bayesian neural networks with Tensorflow and Tensorflow Probability
                                </h3>
                                <p>
                                    NLP & AI Day 2021 (Oct 26, 2021)
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                    <!-- <div>
                        <h4>
                            Archive
                            <p>
                            </p>
                        </h4>
                    </div>

                <div>
                    <ul>

                        <li>
                            <b>What should I buy next? How to leverage word embeddings to build an efficient recommender system.</b>
                            <br>
                            <a href="https://ml.dssconf.pl/">Data Science Summit ML Edition 2021</a>
                            <br><br>
                        </li>

                        <li>
                            <b>Attention! How to efficiently mask sensitive information in unstructured text data?</b>
                            <br>
                            <a href="https://ml.dssconf.pl/">Data Science Summit ML Edition 2021</a>
                            <br><br>
                        </li>

                        <li>
                            <b>Can I do it for you? A case study of the intelligent data imputation system in Master Data Management</b>
                            <br>
                            <a href="https://www.stibosystems.com/connect2020-welcome/">Stibo Global Connect 2020</a>
                            <br><br>                            
                        </li>

                    </ul>

                </div> -->
            </div>
        </div>
        <!-- Service End -->
        
        
        <!-- Banner Start -->
        <div class="banner wow zoomIn" data-wow-delay="0.1s">
            <div class="container">
                <div class="section-header text-center">
                    <p>Let's stay in touch!</p>
                    <h2>Let's connect!</h2>
                </div>
                <div class="container banner-text">
                    <p>
                        I'd love to invite you to my LinkedIn network. Join the community around Sunday AI Papers and connect to thousands of machine learning researchers and practitioners!
                    </p>
                    <a class="btn" href="https://www.linkedin.com/in/aleksandermolak/"></i>Connect</a>
                </div>
            </div>
        </div>
        <!-- Banner End -->


        <!-- Blog Start -->
        <div class="blog" id="blog">
            <div class="container">
                <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
                    <p>Sunday AI Papers</p>
                    <h2>Latest posts</h2>
                </div>
                <div class="row">

                    <!-- Posts start here L -> R -->

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-14.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Can one learn a ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—² of a ğ—°ğ—®ğ˜‚ğ˜€ğ—®ğ—¹ ğ—´ğ—¿ğ—®ğ—½ğ—µ with ğ—¹ğ—®ğ˜ğ—²ğ—»ğ˜ ğ˜ƒğ—®ğ—¿ğ—¶ğ—®ğ—¯ğ—¹ğ—²ğ˜€?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Causality, causal discovery</p>
                                    <p><i class="far fa-calendar-alt"></i>28-Nov-2021</p>
                                </div>
                                <p>
                                    Last Sunday, researchers from University of Chicago and Carnegie Mellon University 
                                    released a paper proposing a novel method of discovering a causal graph with latent 
                                    variables, but the problem is hard.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-causality-activity-6870844568338804736-QvWR">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-13.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>ğ——ğ—¶ğ˜€ğ˜ğ—¶ğ—¹ğ—¹ğ—¶ğ—»ğ—´ ğ—•ğ—˜ğ—¥ğ—§ better? Get ğ—³ğ—®ğ˜€ğ˜ğ—²ğ—¿ and more ğ—¿ğ—¼ğ—¯ğ˜‚ğ˜€ğ˜!</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, distillation, compression</p>
                                    <p><i class="far fa-calendar-alt"></i>21-Nov-2021</p>
                                </div>
                                <p>
                                    Last Thursday, researchers from Intel Labs and University of California, Santa Barbara 
                                    proposed a new approach to model distillation. 
                                    The proposed architecture achieves very good trade-off between ğ—¶ğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ˜ğ—¶ğ—ºğ—² ğ—¿ğ—²ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» and ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜†.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-nlp-activity-6868300191968100352-BVP9">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-12.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Contrastive loss ğ—•ğ—˜ğ—¥ğ—§ pre-training for ğ—¶ğ—ºğ—½ğ—¿ğ—¼ğ˜ƒğ—²ğ—± ğ—¿ğ—²ğ—½ğ—¿ğ—²ğ˜€ğ—²ğ—»ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€? Yes, please!</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, contrastive loss, pre-training</p>
                                    <p><i class="far fa-calendar-alt"></i>14-Nov-2021</p>
                                </div>
                                <p>
                                    Last Tuesday, researchers from University of Cambridge, Amazon Web Services (AWS) AI and 
                                    Monash University released a paper introducing a new pre-training approach leveraging 
                                    contrastive loss scheme. 
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-nlp-activity-6865758937896022016-mDLY">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>
                    
                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-11.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Ok, computer, ğ—½ğ—¿ğ—²ğ—½ğ—¿ğ—¼ğ—°ğ—²ğ˜€ğ˜€ my ğ˜€ğ˜ğ—¿ğ—¶ğ—»ğ—´ğ˜€! ğŸ¤–</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, strings, preprocessing</p>
                                    <p><i class="far fa-calendar-alt"></i>07-Nov-2021</p>
                                </div>
                                <p>
                                    Last Thursday researchers from Eindhoven University of Technology released a paper describing a 
                                    framework for automated string preprocessing and encoding. 
                                    The framework leverages probabilistic type inference among other interesting components.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-sundayaipapers-machinelearning-activity-6863146269716598784-v7Hr">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-10.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Is ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—¼ğ—¯ğ˜€ğ—¼ğ—¹ğ—²ğ˜ğ—²? Can we ğ—½ğ—¿ğ—²ğ—±ğ—¶ğ—°ğ˜ the ğ˜„ğ—²ğ—¶ğ—´ğ—µğ˜ğ˜€ instead?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Parameter search, optimization, GNNs</p>
                                    <p><i class="far fa-calendar-alt"></i>31-Oct-2021</p>
                                </div>
                                <p>
                                    Last Monday, researchers from University of Guelph and Facebook AI 
                                    released a paper presenting a novel framework that predicts parameters 
                                    for an arbitrary network**. What's behind?
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ai-activity-6860681839154135040-lnFc">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-9.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2> Can you broaden my perspective? ğ—–ğ—¼ğ—»ğ˜ğ—¿ğ—®ğ˜€ğ˜ğ—¶ğ˜ƒğ—² Document Representation Learning with ğ—šğ—¿ğ—®ğ—½ğ—µ ğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—» ğ—¡ğ—²ğ˜ğ˜„ğ—¼ğ—¿ğ—¸ğ˜€ over ğ—½ğ—¿ğ—²-ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—²ğ—± ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ğ˜€.</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, contrastive learning, GNNs</p>
                                    <p><i class="far fa-calendar-alt"></i>24-Oct-2021</p>
                                </div>
                                <p>
                                    Last Wednesday, researchers from AWS AI Labs released a paper presenting 
                                    a novel method for ğ—±ğ—¼ğ—°ğ˜‚ğ—ºğ—²ğ—»ğ˜ ğ—¿ğ—²ğ—½ğ—¿ğ—²ğ˜€ğ—²ğ—»ğ˜ğ—®ğ˜ğ—¶ğ—¼ğ—». The method uses a large language model, 
                                    a graph attention neural network and a contrastive training strategy. 
                                    Obtained document representations seem to have ğ˜ƒğ—²ğ—¿ğ˜† ğ—´ğ—¼ğ—¼ğ—± ğ—½ğ—¿ğ—¼ğ—½ğ—²ğ—¿ğ˜ğ—¶ğ—²ğ˜€.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6858125702483496960-Tdh9">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-8.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Hey ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ğ˜€, how ğ˜€ğ—²ğ—»ğ˜€ğ—¶ğ˜ğ—¶ğ˜ƒğ—² are you to ğ˜€ğ—½ğ˜‚ğ—¿ğ—¶ğ—¼ğ˜‚ğ˜€ perturbations?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, causality</p>
                                    <p><i class="far fa-calendar-alt"></i>17-Oct-2021</p>
                                </div>
                                <p>
                                    The ties between NLP and Causal Inference grow stronger. 
                                    Last Thursday, a group of researchers from Peking University and 
                                    Normal University of Singapore released a paper analysing four 
                                    NLP architectures' sensitivity to spurious features using Causal Inference tools.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6855600302754713600-Qfq3">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".6s">
                            <div class="blog-img">
                                <img src="img/blog-7.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Data collection, ğ—°ğ—®ğ˜‚ğ˜€ğ—®ğ—¹ğ—¶ğ˜ğ˜† and ğ—¡ğ—Ÿğ—£?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, causality</p>
                                    <p><i class="far fa-calendar-alt"></i>10-Oct-2021</p>
                                </div>
                                <p>
                                    Last Friday researchers from Max Planck Institute for Intelligent Systems, 
                                    ETH ZÃ¼rich, University of Cambridge, UCL and 
                                    Indian Institute of Technology, Kharagpur published a paper 
                                    examining a relationship between data collection process and models 
                                    performance on various NLP tasks. 
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6853075833230286848-sn38">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".6s">
                            <div class="blog-img">
                                <img src="img/blog-6.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Does ğ—•ğ—˜ğ—¥ğ—§ really ğ—±ğ—¼ what ğ˜†ğ—¼ğ˜‚ ğ˜ğ—µğ—¶ğ—»ğ—¸ it ğ—±ğ—¼ğ—²ğ˜€?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, BERT, permutation</p>
                                    <p><i class="far fa-calendar-alt"></i>03-Oct-2021</p>
                                </div>
                                <p>
                                    Last week, we've seen how ğ—½ğ—¼ğ—¼ğ—¿ğ—¹ğ˜† BERT might ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ—¹ğ—¶ğ˜‡ğ—² 
                                    in named entity recognition (ğ—¡ğ—˜ğ—¥) tasks (https://lnkd.in/e-zfBNg9). 
                                    This week we'll look at a paper released on Tuesday, 
                                    where researchers from Sberbank and Higher School of Economics 
                                    in Moscow systematically examine ğ˜€ğ˜†ğ—»ğ˜ğ—®ğ—°ğ˜ğ—¶ğ—° ğ—®ğ—¯ğ—¶ğ—¹ğ—¶ğ˜ğ—¶ğ—²ğ˜€ of the model.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6850491510715146241-TCai">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".9s">
                            <div class="blog-img">
                                <img src="img/blog-5.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>ğ—›ğ—¼ğ˜„ ğ—²ğ—®ğ˜€ğ—¶ğ—¹ğ˜† could you ğ—ºğ—¶ğ˜€ğ—¹ğ—²ğ—®ğ—± ğ—•ğ—˜ğ—¥ğ—§ in recognizing ğ—»ğ—®ğ—ºğ—²ğ—± ğ—²ğ—»ğ˜ğ—¶ğ˜ğ—¶ğ—²ğ˜€ (NER)?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, adversarial attacks</p>
                                    <p><i class="far fa-calendar-alt"></i>26-Sep-2021</p>
                                </div>
                                <p>
                                    Last Thursday researchers from Leiden University released a paper 
                                    examining the influence of ğ—®ğ—±ğ˜ƒğ—²ğ—¿ğ˜€ğ—®ğ—¿ğ—¶ğ—®ğ—¹ ğ—®ğ˜ğ˜ğ—®ğ—°ğ—¸ğ˜€ on ğ—•ğ—˜ğ—¥ğ—§'s capabilities in 
                                    ğ—¡ğ—®ğ—ºğ—²ğ—± ğ—˜ğ—»ğ˜ğ—¶ğ˜ğ˜† ğ—¥ğ—²ğ—°ğ—¼ğ—´ğ—»ğ—¶ğ˜ğ—¶ğ—¼ğ—» (NER).
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6847996331672297473-OhLo">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.9s">
                            <div class="blog-img">
                                <img src="img/blog-4.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Can ğ—¿ğ—®ğ—»ğ—±ğ—¼ğ—ºğ—¹ğ˜†-ğ˜„ğ—²ğ—¶ğ—´ğ—µğ˜ğ—²ğ—± one-layer ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ be as good as a ğ—³ğ˜‚ğ—¹ğ—¹ğ˜† ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—²ğ—± model?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, memory efficiency</p>
                                    <p><i class="far fa-calendar-alt"></i>12-Sep-2021</p>
                                </div>
                                <p>
                                    Last Wednesday researchers from Facebook AI and University of California, Berkeley 
                                    released a paper examining the potential of randomly-weighted one-layer 
                                    Transformer for translation and other NLP tasks.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6842896382714548224-HPG-">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.6s">
                            <div class="blog-img">
                                <img src="img/blog-3.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Who wants an ğ—²ğ—³ğ—³ğ—¶ğ—°ğ—¶ğ—²ğ—»ğ˜ ğ˜ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ for ğ—®ğ—¿ğ—¯ğ—¶ğ˜ğ—¿ğ—®ğ—¿ğ—¶ğ—¹ğ˜† ğ—¹ğ—¼ğ—»ğ—´ ğ˜€ğ—²ğ—¾ğ˜‚ğ—²ğ—»ğ—°ğ—²ğ˜€? </h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, memory efficiency</p>
                                    <p><i class="far fa-calendar-alt"></i>05-Sep-2021</p>
                                </div>
                                <p>
                                    Last Wednesday researchers from University of Lisbon and DeepMind released 
                                    a paper where they proposed an infinite-memory transformer or âˆ-ğ™›ğ™¤ğ™§ğ™¢ğ™šğ™§. 
                                    The model is able to model arbitrarily long contexts.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6840375144599367680-IACv">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.3s">
                            <div class="blog-img">
                                <img src="img/blog-2.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Have you ever heard of ğ˜ƒğ—®ğ—¿ğ—¶ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ—®ğ˜‚ğ˜ğ—¼ğ—²ğ—»ğ—°ğ—¼ğ—±ğ—²ğ—¿ğ˜€ (VAEs) for ğ—¡ğ—Ÿğ—£?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, variational inference</p>
                                    <p><i class="far fa-calendar-alt"></i>29-Aug-2021</p>
                                </div>
                                <p>
                                    Last Monday researchers from Universidad Carlos III de Madrid and Swiss Data Science Institute (ETHZ/EPFL) 
                                    released a paper, proposing a novel and surprising usage of ğ—©ğ—”ğ—˜s within ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ğ˜€ framework. 
                                    ğ—¡ğ—¼ğ—¥ğ—•ğ—˜ğ—¥ğ—§ is a modified ğ—•ğ—˜ğ—¥ğ—§ architecture with a special VAE component.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6837843979887808512-jAUO">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.1s">
                            <div class="blog-img">
                                <img src="img/blog-1.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Dreaming about ğ—¿ğ—¼ğ—¯ğ˜‚ğ˜€ğ˜ ğ—¡ğ—˜ğ—¥ models with ğ—½ğ—®ğ—¿ğ˜ğ—¶ğ—®ğ—¹ğ—¹ğ˜† ğ—¹ğ—®ğ—¯ğ—²ğ—¹ğ—¹ğ—²ğ—± data?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, NER, Transformers</p>
                                    <p><i class="far fa-calendar-alt"></i>22-Aug-2021</p>
                                </div>
                                <p>
                                    Last week researchers from Columbia University in the City of New York and 
                                    Google Research released a paper proposing a new approach towards partially 
                                    supervised named entity recognition (NER).
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6835313599288799232-FY-R">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>




                </div>
            </div>
        </div>
        <!-- Blog End -->


        <!-- Footer Start -->
        <div class="footer wow fadeIn" data-wow-delay="0.3s" id="contact">
            <div class="container-fluid">
                <div class="container">
                    <div class="footer-info">
                        <h2>Let's connect!</h2>
                        <div class="footer-social">
                            <a href="https://www.linkedin.com/in/aleksandermolak"><i class="fab fa-linkedin-in"></i></a>
                            <a href="https://github.com/AlxndrMlk"><i class="fab fa-github"></i></a>
                            <a href="https://twitter.com/AleksanderMolak"><i class="fab fa-twitter"></i></a>
                            <a href="https://www.researchgate.net/profile/Aleksander-Molak"><i class="fab fa-researchgate"></i></a>
                            <!-- <a href=""><i class="fab fa-youtube"></i></a>
                            <a href=""><i class="fab fa-instagram"></i></a> -->
                        </div>
                    </div>
                </div>
                <div class="container copyright">
                    <p>&copy; <a href="#">alxndr.io 2021</a></p>
                    <p class='small-letters'>Based on a template from <a href="https://htmlcodex.com">HTML Codex</a></p>
                </div>
            </div>
        </div>
        <!-- Footer End -->
        
        
        <!-- Back to top button -->
        <a href="#" class="btn back-to-top"><i class="fa fa-chevron-up"></i></a>
        
        
        <!-- Pre Loader -->
        <div id="loader" class="show">
            <div class="loader"></div>
        </div>

        
        <!-- JavaScript Libraries -->
        <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>
        <script src="lib/easing/easing.min.js"></script>
        <script src="lib/wow/wow.min.js"></script>
        <script src="lib/waypoints/waypoints.min.js"></script>
        <script src="lib/typed/typed.min.js"></script>
        <script src="lib/owlcarousel/owl.carousel.min.js"></script>
        <script src="lib/isotope/isotope.pkgd.min.js"></script>
        <script src="lib/lightbox/js/lightbox.min.js"></script>
        
        <!-- Contact Javascript File -->
        <script src="mail/jqBootstrapValidation.min.js"></script>
        <script src="mail/contact.js"></script>

        <!-- Template Javascript -->
        <script src="js/main.js"></script>
    </body>
</html>
