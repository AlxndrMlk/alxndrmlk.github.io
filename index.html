<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>alxndr.io - Aleksander Molak</title>
        <meta content="width=device-width, initial-scale=1.0" name="viewport">
        <meta content="
        aleksander molak, 
        machine learning, 
        artificial intelligence, 
        ml, 
        ai, 
        natural language processing, 
        nlp,
        probabilistic machine learning,
        bayesian,
        statistics,
        causality,
        inference,
        business,
        consulting
        " name="keywords">
        <meta content="Aleksander Molak's personal site. ML, AI, Machine Learning, NLP, Bayesian methods" name="description">

        <!-- Favicon -->
        <link href="img/favicon.ico" rel="icon">

        <!-- Google Font -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@500&display=swap" rel="stylesheet"> 

        <!-- CSS Libraries -->
        <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.10.0/css/all.min.css" rel="stylesheet">
        <link href="lib/animate/animate.min.css" rel="stylesheet">
        <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">
        <link href="lib/lightbox/css/lightbox.min.css" rel="stylesheet">

        <!-- Template Stylesheet -->
        <link href="css/style.css" rel="stylesheet">


        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-MNGYMD6CC6"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-MNGYMD6CC6');
        </script>
    </head>

    <body data-spy="scroll" data-target=".navbar" data-offset="51">
        <!-- Nav Bar Start -->
        <div class="navbar navbar-expand-lg bg-light navbar-light">
            <div class="container-fluid">
                <a href="index.html" class="navbar-brand">alxndr<span class="navbar-brand-dot">.</span>io</a>
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbarCollapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <div class="collapse navbar-collapse justify-content-between" id="navbarCollapse">
                    <div class="navbar-nav ml-auto">
                        <a href="#home" class="nav-item nav-link active">Home</a>
                        <a href="#about" class="nav-item nav-link">About</a>
                        <a href="#service" class="nav-item nav-link">Talks & Workshops</a>
                        <!-- <a href="#experience" class="nav-item nav-link">Experience</a> -->
                        <!-- <a href="#portfolio" class="nav-item nav-link"></a> -->
                        <!-- <a href="#price" class="nav-item nav-link">Price</a> -->
                        <!-- <a href="#review" class="nav-item nav-link">Review</a> -->
                        <!-- <a href="#team" class="nav-item nav-link">Team</a> -->
                        <a href="#blog" class="nav-item nav-link">Sunday AI Papers</a>
                        <a href="#contact" class="nav-item nav-link">Contact</a>
                    </div>
                </div>
            </div>
        </div>
        <!-- Nav Bar End -->


        <!-- Hero Start -->
        <div class="hero" id="home">
            <div class="container-fluid">
                <div class="row align-items-center">
                    <div class="col-sm-12 col-md-6">
                        <div class="hero-content">
                            <div class="hero-text add-margin-top">
                                <!-- <p>אלכסנדר מולק</p> -->
                                <h1>Alexander Molak</h1>
                                <h2></h2>
                                <div class="typed-text">Machine Learning, NLP, Causal Inference, Probabilistic Modeling, Psychology, Business</div>
                            </div>
                            <div class="hero-btn">
                                <a class="btn add-margin-bottom" href="https://www.linkedin.com/in/aleksandermolak/"><i class="fab fa-linkedin-in"> </i> LinkedIn</a>
                                <a class="btn add-margin-bottom" href="https://github.com/AlxndrMlk"><i class="fab fa-github"></i> GitHub</a>
                            </div>
                        </div>
                    </div>
                    <div class="col-sm-12 col-md-6 d-none d-md-block">
                        <div class="hero-image">
                            <img src="img/hero_sm.png" alt="Hero Image">
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Hero End -->


        <!-- About Start -->
        <div class="about wow fadeInUp" data-wow-delay="0.1s" id="about">
            <div class="container-fluid">
                <div class="row align-items-center">
                    <div class="col-lg-6">
                        <div class="about-img">
                            <img src="img/about_sm.jpg" alt="Image">
                        </div>
                    </div>
                    <div class="col-lg-6">
                        <div class="about-content">
                            <div class="section-header text-left">
                                <p>About Me</p>
                                <h2>Crossing the boundaries</h2>
                            </div>
                            <div class="about-text">
                                <p>
                                    I am an Innovation Lead and Machine Learning Researcher at <a href="https://lingarogroup.com/">Lingaro</a>, 
                                    where I build production-grade machine learning systems for Fortune 100 companies.
                                </p>

                                <p>
                                    I am specialized in natural language processing (NLP), causal inference and probabilistic modeling. 
                                </p>

                                <p>
                                    My academic background is in philosophy of language and experimental psychology.
                                    Before starting my career in data science, I used to work as a music producer and mixing engineer. 
                                </p>
                                <p>
                                    Every week on Sunday evening I write a micro-review of a recent paper in machine learning as a part of my <a href="https://www.linkedin.com/in/aleksandermolak/"> LinkedIn</a> project 
                                    <a href="https://www.linkedin.com/feed/hashtag/?keywords=sundayaipapers"><b>Sunday AI Papers</b></a>.
                                </p>
                            </div>
                            <a class="btn" href="#blog">Learn more</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- About End -->
        
        
        <!-- Service Start -->
        <div class="service" id="service">
            <div class="container">
                <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
                    <p>Talks & Workshops</p>
                    <h2>Sharing knowledge</h2>
                </div>

                <p>
                    I am extremely grateful to all the people who shared their knowledge and experience with me. 
                    It's very important to me to give back what I got from others. That's why I love to <b>share</b> my knowledge and experience with others.
                </p>
                <p>
                    Interested in NLP, causal or probabilistic modeling? <b>Join me</b> on one of the upcoming events!<br>
                </p>
                <p>
                    -
                </p>

                <div>
                    <h4>
                        Upcoming events
                        <p></p>
                    </h4>
                </div>

                <div class="row">

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://dssconf.pl/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Causal Inference in Python: An Introduction</h3>
                                <p>
                                    Data Science Summit (Dec 3, 2021)
                                </p>
                            </div>
                        </div>
                    </div>

                </div>

                <div>
                    <h4>
                        Past events
                        <p></p>
                    </h4>
                </div>

                <div class="row">

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://www.meetup.com/PyData-Tel-Aviv/events/281354553/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>
                                    What should I buy next? How to leverage word embeddings to build an efficient recommender system
                                </h3>
                                    
                                <p>
                                    PyData Tel Aviv 2021 (Nov 10, 2021)
                                </p>
                                <p>
                                    <a href="https://www.youtube.com/watch?v=_IZUmV28JpA"><i class="fab fa-youtube" style="color: red;"></i></a>
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://pydata.org/global2021/schedule/presentation/13/modeling-aleatoric-and-epistemic-uncertainty-using-tensorflow-and-tensorflow-probability/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Modeling aleatoric and epistemic uncertainty using Tensorflow and Tensorflow Probability</h3>
                                <p>
                                    PyData Global 2021 (Oct 30, 2021)
                                </p>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.2s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://www.nlpday.pl/" title="Register"><i class="fas fa-code"></i></a>
                            </div>
                            
                            <div class="service-text">
                                <h3>
                                    Uncertainty? Hands-on Bayesian neural networks with Tensorflow and Tensorflow Probability
                                </h3>
                                <p>
                                    NLP & AI Day 2021 (Oct 26, 2021)
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                    <!-- <div>
                        <h4>
                            Archive
                            <p>
                            </p>
                        </h4>
                    </div>

                <div>
                    <ul>

                        <li>
                            <b>What should I buy next? How to leverage word embeddings to build an efficient recommender system.</b>
                            <br>
                            <a href="https://ml.dssconf.pl/">Data Science Summit ML Edition 2021</a>
                            <br><br>
                        </li>

                        <li>
                            <b>Attention! How to efficiently mask sensitive information in unstructured text data?</b>
                            <br>
                            <a href="https://ml.dssconf.pl/">Data Science Summit ML Edition 2021</a>
                            <br><br>
                        </li>

                        <li>
                            <b>Can I do it for you? A case study of the intelligent data imputation system in Master Data Management</b>
                            <br>
                            <a href="https://www.stibosystems.com/connect2020-welcome/">Stibo Global Connect 2020</a>
                            <br><br>                            
                        </li>

                    </ul>

                </div> -->
            </div>
        </div>
        <!-- Service End -->
        
        
        <!-- Banner Start -->
        <div class="banner wow zoomIn" data-wow-delay="0.1s">
            <div class="container">
                <div class="section-header text-center">
                    <p>Let's stay in touch!</p>
                    <h2>Let's connect!</h2>
                </div>
                <div class="container banner-text">
                    <p>
                        I'd love to invite you to my LinkedIn network. Join the community around Sunday AI Papers and connect to thousands of machine learning researchers and practitioners!
                    </p>
                    <a class="btn" href="https://www.linkedin.com/in/aleksandermolak/"></i>Connect</a>
                </div>
            </div>
        </div>
        <!-- Banner End -->


        <!-- Blog Start -->
        <div class="blog" id="blog">
            <div class="container">
                <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
                    <p>Sunday AI Papers</p>
                    <h2>Latest posts</h2>
                </div>
                <div class="row">

                    <!-- Posts start here L -> R -->

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-14.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Can one learn a 𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲 of a 𝗰𝗮𝘂𝘀𝗮𝗹 𝗴𝗿𝗮𝗽𝗵 with 𝗹𝗮𝘁𝗲𝗻𝘁 𝘃𝗮𝗿𝗶𝗮𝗯𝗹𝗲𝘀?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Causality, causal discovery</p>
                                    <p><i class="far fa-calendar-alt"></i>28-Nov-2021</p>
                                </div>
                                <p>
                                    Last Sunday, researchers from University of Chicago and Carnegie Mellon University 
                                    released a paper proposing a novel method of discovering a causal graph with latent 
                                    variables, but the problem is hard.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-causality-activity-6870844568338804736-QvWR">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-13.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>𝗗𝗶𝘀𝘁𝗶𝗹𝗹𝗶𝗻𝗴 𝗕𝗘𝗥𝗧 better? Get 𝗳𝗮𝘀𝘁𝗲𝗿 and more 𝗿𝗼𝗯𝘂𝘀𝘁!</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, distillation, compression</p>
                                    <p><i class="far fa-calendar-alt"></i>21-Nov-2021</p>
                                </div>
                                <p>
                                    Last Thursday, researchers from Intel Labs and University of California, Santa Barbara 
                                    proposed a new approach to model distillation. 
                                    The proposed architecture achieves very good trade-off between 𝗶𝗻𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝘁𝗶𝗺𝗲 𝗿𝗲𝗱𝘂𝗰𝘁𝗶𝗼𝗻 and 𝗮𝗰𝗰𝘂𝗿𝗮𝗰𝘆.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-nlp-activity-6868300191968100352-BVP9">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-12.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Contrastive loss 𝗕𝗘𝗥𝗧 pre-training for 𝗶𝗺𝗽𝗿𝗼𝘃𝗲𝗱 𝗿𝗲𝗽𝗿𝗲𝘀𝗲𝗻𝗮𝘁𝗶𝗼𝗻𝘀? Yes, please!</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, contrastive loss, pre-training</p>
                                    <p><i class="far fa-calendar-alt"></i>14-Nov-2021</p>
                                </div>
                                <p>
                                    Last Tuesday, researchers from University of Cambridge, Amazon Web Services (AWS) AI and 
                                    Monash University released a paper introducing a new pre-training approach leveraging 
                                    contrastive loss scheme. 
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-nlp-activity-6865758937896022016-mDLY">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>
                    
                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-11.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Ok, computer, 𝗽𝗿𝗲𝗽𝗿𝗼𝗰𝗲𝘀𝘀 my 𝘀𝘁𝗿𝗶𝗻𝗴𝘀! 🤖</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, strings, preprocessing</p>
                                    <p><i class="far fa-calendar-alt"></i>07-Nov-2021</p>
                                </div>
                                <p>
                                    Last Thursday researchers from Eindhoven University of Technology released a paper describing a 
                                    framework for automated string preprocessing and encoding. 
                                    The framework leverages probabilistic type inference among other interesting components.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-sundayaipapers-machinelearning-activity-6863146269716598784-v7Hr">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-10.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Is 𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗼𝗯𝘀𝗼𝗹𝗲𝘁𝗲? Can we 𝗽𝗿𝗲𝗱𝗶𝗰𝘁 the 𝘄𝗲𝗶𝗴𝗵𝘁𝘀 instead?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Parameter search, optimization, GNNs</p>
                                    <p><i class="far fa-calendar-alt"></i>31-Oct-2021</p>
                                </div>
                                <p>
                                    Last Monday, researchers from University of Guelph and Facebook AI 
                                    released a paper presenting a novel framework that predicts parameters 
                                    for an arbitrary network**. What's behind?
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ai-activity-6860681839154135040-lnFc">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-9.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2> Can you broaden my perspective? 𝗖𝗼𝗻𝘁𝗿𝗮𝘀𝘁𝗶𝘃𝗲 Document Representation Learning with 𝗚𝗿𝗮𝗽𝗵 𝗔𝘁𝘁𝗲𝗻𝘁𝗶𝗼𝗻 𝗡𝗲𝘁𝘄𝗼𝗿𝗸𝘀 over 𝗽𝗿𝗲-𝘁𝗿𝗮𝗶𝗻𝗲𝗱 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿𝘀.</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, contrastive learning, GNNs</p>
                                    <p><i class="far fa-calendar-alt"></i>24-Oct-2021</p>
                                </div>
                                <p>
                                    Last Wednesday, researchers from AWS AI Labs released a paper presenting 
                                    a novel method for 𝗱𝗼𝗰𝘂𝗺𝗲𝗻𝘁 𝗿𝗲𝗽𝗿𝗲𝘀𝗲𝗻𝘁𝗮𝘁𝗶𝗼𝗻. The method uses a large language model, 
                                    a graph attention neural network and a contrastive training strategy. 
                                    Obtained document representations seem to have 𝘃𝗲𝗿𝘆 𝗴𝗼𝗼𝗱 𝗽𝗿𝗼𝗽𝗲𝗿𝘁𝗶𝗲𝘀.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6858125702483496960-Tdh9">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-8.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Hey 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿𝘀, how 𝘀𝗲𝗻𝘀𝗶𝘁𝗶𝘃𝗲 are you to 𝘀𝗽𝘂𝗿𝗶𝗼𝘂𝘀 perturbations?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, causality</p>
                                    <p><i class="far fa-calendar-alt"></i>17-Oct-2021</p>
                                </div>
                                <p>
                                    The ties between NLP and Causal Inference grow stronger. 
                                    Last Thursday, a group of researchers from Peking University and 
                                    Normal University of Singapore released a paper analysing four 
                                    NLP architectures' sensitivity to spurious features using Causal Inference tools.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6855600302754713600-Qfq3">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".6s">
                            <div class="blog-img">
                                <img src="img/blog-7.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Data collection, 𝗰𝗮𝘂𝘀𝗮𝗹𝗶𝘁𝘆 and 𝗡𝗟𝗣?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, causality</p>
                                    <p><i class="far fa-calendar-alt"></i>10-Oct-2021</p>
                                </div>
                                <p>
                                    Last Friday researchers from Max Planck Institute for Intelligent Systems, 
                                    ETH Zürich, University of Cambridge, UCL and 
                                    Indian Institute of Technology, Kharagpur published a paper 
                                    examining a relationship between data collection process and models 
                                    performance on various NLP tasks. 
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6853075833230286848-sn38">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".6s">
                            <div class="blog-img">
                                <img src="img/blog-6.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Does 𝗕𝗘𝗥𝗧 really 𝗱𝗼 what 𝘆𝗼𝘂 𝘁𝗵𝗶𝗻𝗸 it 𝗱𝗼𝗲𝘀?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, BERT, permutation</p>
                                    <p><i class="far fa-calendar-alt"></i>03-Oct-2021</p>
                                </div>
                                <p>
                                    Last week, we've seen how 𝗽𝗼𝗼𝗿𝗹𝘆 BERT might 𝗴𝗲𝗻𝗲𝗿𝗮𝗹𝗶𝘇𝗲 
                                    in named entity recognition (𝗡𝗘𝗥) tasks (https://lnkd.in/e-zfBNg9). 
                                    This week we'll look at a paper released on Tuesday, 
                                    where researchers from Sberbank and Higher School of Economics 
                                    in Moscow systematically examine 𝘀𝘆𝗻𝘁𝗮𝗰𝘁𝗶𝗰 𝗮𝗯𝗶𝗹𝗶𝘁𝗶𝗲𝘀 of the model.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6850491510715146241-TCai">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".9s">
                            <div class="blog-img">
                                <img src="img/blog-5.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>𝗛𝗼𝘄 𝗲𝗮𝘀𝗶𝗹𝘆 could you 𝗺𝗶𝘀𝗹𝗲𝗮𝗱 𝗕𝗘𝗥𝗧 in recognizing 𝗻𝗮𝗺𝗲𝗱 𝗲𝗻𝘁𝗶𝘁𝗶𝗲𝘀 (NER)?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, adversarial attacks</p>
                                    <p><i class="far fa-calendar-alt"></i>26-Sep-2021</p>
                                </div>
                                <p>
                                    Last Thursday researchers from Leiden University released a paper 
                                    examining the influence of 𝗮𝗱𝘃𝗲𝗿𝘀𝗮𝗿𝗶𝗮𝗹 𝗮𝘁𝘁𝗮𝗰𝗸𝘀 on 𝗕𝗘𝗥𝗧's capabilities in 
                                    𝗡𝗮𝗺𝗲𝗱 𝗘𝗻𝘁𝗶𝘁𝘆 𝗥𝗲𝗰𝗼𝗴𝗻𝗶𝘁𝗶𝗼𝗻 (NER).
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6847996331672297473-OhLo">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.9s">
                            <div class="blog-img">
                                <img src="img/blog-4.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Can 𝗿𝗮𝗻𝗱𝗼𝗺𝗹𝘆-𝘄𝗲𝗶𝗴𝗵𝘁𝗲𝗱 one-layer 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿 be as good as a 𝗳𝘂𝗹𝗹𝘆 𝘁𝗿𝗮𝗶𝗻𝗲𝗱 model?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, memory efficiency</p>
                                    <p><i class="far fa-calendar-alt"></i>12-Sep-2021</p>
                                </div>
                                <p>
                                    Last Wednesday researchers from Facebook AI and University of California, Berkeley 
                                    released a paper examining the potential of randomly-weighted one-layer 
                                    Transformer for translation and other NLP tasks.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6842896382714548224-HPG-">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.6s">
                            <div class="blog-img">
                                <img src="img/blog-3.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Who wants an 𝗲𝗳𝗳𝗶𝗰𝗶𝗲𝗻𝘁 𝘁𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿 for 𝗮𝗿𝗯𝗶𝘁𝗿𝗮𝗿𝗶𝗹𝘆 𝗹𝗼𝗻𝗴 𝘀𝗲𝗾𝘂𝗲𝗻𝗰𝗲𝘀? </h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, memory efficiency</p>
                                    <p><i class="far fa-calendar-alt"></i>05-Sep-2021</p>
                                </div>
                                <p>
                                    Last Wednesday researchers from University of Lisbon and DeepMind released 
                                    a paper where they proposed an infinite-memory transformer or ∞-𝙛𝙤𝙧𝙢𝙚𝙧. 
                                    The model is able to model arbitrarily long contexts.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6840375144599367680-IACv">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.3s">
                            <div class="blog-img">
                                <img src="img/blog-2.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Have you ever heard of 𝘃𝗮𝗿𝗶𝗮𝘁𝗶𝗼𝗻𝗮𝗹 𝗮𝘂𝘁𝗼𝗲𝗻𝗰𝗼𝗱𝗲𝗿𝘀 (VAEs) for 𝗡𝗟𝗣?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, variational inference</p>
                                    <p><i class="far fa-calendar-alt"></i>29-Aug-2021</p>
                                </div>
                                <p>
                                    Last Monday researchers from Universidad Carlos III de Madrid and Swiss Data Science Institute (ETHZ/EPFL) 
                                    released a paper, proposing a novel and surprising usage of 𝗩𝗔𝗘s within 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿𝘀 framework. 
                                    𝗡𝗼𝗥𝗕𝗘𝗥𝗧 is a modified 𝗕𝗘𝗥𝗧 architecture with a special VAE component.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6837843979887808512-jAUO">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.1s">
                            <div class="blog-img">
                                <img src="img/blog-1.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Dreaming about 𝗿𝗼𝗯𝘂𝘀𝘁 𝗡𝗘𝗥 models with 𝗽𝗮𝗿𝘁𝗶𝗮𝗹𝗹𝘆 𝗹𝗮𝗯𝗲𝗹𝗹𝗲𝗱 data?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, NER, Transformers</p>
                                    <p><i class="far fa-calendar-alt"></i>22-Aug-2021</p>
                                </div>
                                <p>
                                    Last week researchers from Columbia University in the City of New York and 
                                    Google Research released a paper proposing a new approach towards partially 
                                    supervised named entity recognition (NER).
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6835313599288799232-FY-R">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>




                </div>
            </div>
        </div>
        <!-- Blog End -->


        <!-- Footer Start -->
        <div class="footer wow fadeIn" data-wow-delay="0.3s" id="contact">
            <div class="container-fluid">
                <div class="container">
                    <div class="footer-info">
                        <h2>Let's connect!</h2>
                        <div class="footer-social">
                            <a href="https://www.linkedin.com/in/aleksandermolak"><i class="fab fa-linkedin-in"></i></a>
                            <a href="https://github.com/AlxndrMlk"><i class="fab fa-github"></i></a>
                            <a href="https://twitter.com/AleksanderMolak"><i class="fab fa-twitter"></i></a>
                            <a href="https://www.researchgate.net/profile/Aleksander-Molak"><i class="fab fa-researchgate"></i></a>
                            <!-- <a href=""><i class="fab fa-youtube"></i></a>
                            <a href=""><i class="fab fa-instagram"></i></a> -->
                        </div>
                    </div>
                </div>
                <div class="container copyright">
                    <p>&copy; <a href="#">alxndr.io 2021</a></p>
                    <p class='small-letters'>Based on a template from <a href="https://htmlcodex.com">HTML Codex</a></p>
                </div>
            </div>
        </div>
        <!-- Footer End -->
        
        
        <!-- Back to top button -->
        <a href="#" class="btn back-to-top"><i class="fa fa-chevron-up"></i></a>
        
        
        <!-- Pre Loader -->
        <div id="loader" class="show">
            <div class="loader"></div>
        </div>

        
        <!-- JavaScript Libraries -->
        <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>
        <script src="lib/easing/easing.min.js"></script>
        <script src="lib/wow/wow.min.js"></script>
        <script src="lib/waypoints/waypoints.min.js"></script>
        <script src="lib/typed/typed.min.js"></script>
        <script src="lib/owlcarousel/owl.carousel.min.js"></script>
        <script src="lib/isotope/isotope.pkgd.min.js"></script>
        <script src="lib/lightbox/js/lightbox.min.js"></script>
        
        <!-- Contact Javascript File -->
        <script src="mail/jqBootstrapValidation.min.js"></script>
        <script src="mail/contact.js"></script>

        <!-- Template Javascript -->
        <script src="js/main.js"></script>
    </body>
</html>
