<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>alxndr.io - Aleksander Molak</title>
        <meta content="width=device-width, initial-scale=1.0" name="viewport">
        <meta content="
        aleksander molak, 
        machine learning, 
        artificial intelligence, 
        ml, 
        ai, 
        natural language processing, 
        nlp,
        probabilistic machine learning,
        bayesian,
        statistics,
        causality,
        inference,
        business,
        consulting
        " name="keywords">
        <meta content="Aleksander Molak's personal site. ML, AI, Machine Learning, NLP, Bayesian methods" name="description">

        <!-- Favicon -->
        <link href="img/favicon.ico" rel="icon">

        <!-- Google Font -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@500&display=swap" rel="stylesheet"> 

        <!-- CSS Libraries -->
        <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.10.0/css/all.min.css" rel="stylesheet">
        <link href="lib/animate/animate.min.css" rel="stylesheet">
        <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">
        <link href="lib/lightbox/css/lightbox.min.css" rel="stylesheet">

        <!-- Template Stylesheet -->
        <link href="css/style.css" rel="stylesheet">


        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-MNGYMD6CC6"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-MNGYMD6CC6');
        </script>
    </head>

    <body data-spy="scroll" data-target=".navbar" data-offset="51">
        <!-- Nav Bar Start -->
        <div class="navbar navbar-expand-lg bg-light navbar-light">
            <div class="container-fluid">
                <a href="index.html" class="navbar-brand">alxndr<span class="navbar-brand-dot">.</span>io</a>
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbarCollapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <div class="collapse navbar-collapse justify-content-between" id="navbarCollapse">
                    <div class="navbar-nav ml-auto">
                        <a href="#home" class="nav-item nav-link active">Home</a>
                        <a href="#about" class="nav-item nav-link">About</a>
                        <a href="#service" class="nav-item nav-link">Talks & Workshops</a>
                        <!-- <a href="#experience" class="nav-item nav-link">Experience</a> -->
                        <!-- <a href="#portfolio" class="nav-item nav-link"></a> -->
                        <!-- <a href="#price" class="nav-item nav-link">Price</a> -->
                        <!-- <a href="#review" class="nav-item nav-link">Review</a> -->
                        <!-- <a href="#team" class="nav-item nav-link">Team</a> -->
                        <a href="#blogblog" class="nav-item nav-link">Blog</a>
                        <a href="#blog" class="nav-item nav-link">Sunday AI Papers</a>
                        <a href="#contact" class="nav-item nav-link">Contact</a>
                    </div>
                </div>
            </div>
        </div>
        <!-- Nav Bar End -->


        <!-- Hero Start -->
        <div class="hero" id="home">
            <div class="container-fluid">
                <div class="row align-items-center">
                    <div class="col-sm-12 col-md-6">
                        <div class="hero-content">
                            <div class="hero-text add-margin-top">
                                <!-- <p>אלכסנדר מולק</p> -->
                                <h1>Alexander Molak</h1>
                                <h2></h2>
                                <div class="typed-text">Machine Learning, NLP, Causal Inference, Probabilistic Modeling, Psychology, Business</div>
                            </div>
                            <div class="hero-btn">
                                <a class="btn add-margin-bottom" href="https://www.linkedin.com/in/aleksandermolak/"><i class="fab fa-linkedin-in"> </i> LinkedIn</a>
                                <!-- <a class="btn add-margin-bottom" href="https://github.com/AlxndrMlk"><i class="fab fa-github"></i> GitHub</a> -->
                                <a class="btn add-margin-bottom" href="https://aleksander-molak.medium.com/"><i class="fab fa-medium"></i> Medium</a>
                            </div>
                        </div>
                    </div>
                    <div class="col-sm-12 col-md-6 d-none d-md-block">
                        <div class="hero-image">
                            <img src="img/hero_sm.png" alt="Hero Image">
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Hero End -->


        <!-- About Start -->
        <div class="about wow fadeInUp" data-wow-delay="0.1s" id="about">
            <div class="container-fluid">
                <div class="row align-items-center">
                    <div class="col-lg-6">
                        <div class="about-img">
                            <img src="img/about_sm.jpg" alt="Image">
                        </div>
                    </div>
                    <div class="col-lg-6">
                        <div class="about-content">
                            <div class="section-header text-left">
                                <p>About Me</p>
                                <h2>Crossing the boundaries</h2>
                            </div>
                            <div class="about-text">
                                <p>
                                    I am an Innovation Lead and Machine Learning Researcher currently having a break from regular job.
                                </p>

                                <p>
                                    I am specialized in natural language processing (NLP), causal inference and probabilistic modeling. 
                                </p>

                                <p>
                                    My academic background is in philosophy of language and experimental psychology.
                                    Before starting my career in data science, I used to work as a music producer and mixing engineer. 
                                </p>
                                <p>
                                    Every week on Sunday evening I write a micro-review of a recent paper in machine learning as a part of my <a href="https://www.linkedin.com/in/aleksandermolak/"> LinkedIn</a> project 
                                    <a href="https://www.linkedin.com/feed/hashtag/?keywords=sundayaipapers"><b>Sunday AI Papers</b></a>.
                                </p>
                            </div>
                            <a class="btn" href="#blog">Learn more</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- About End -->
        
        
        <!-- Service Start -->
        <div class="service" id="service">
            <div class="container">
                <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
                    <p>Talks & Workshops</p>
                    <h2>Sharing knowledge</h2>
                </div>

                <p>
                    I am extremely grateful to all the people who shared their knowledge and experience with me. 
                    It's very important to me to give back what I got from others. That's why I love to <b>share</b> my knowledge and experience with others.
                </p>
                <p>
                    Interested in NLP, causal or probabilistic modeling? <b>Join me</b> on one of the upcoming events!<br>
                </p>
                <p>
                    -
                </p>

                <div>
                    <h4>
                        Upcoming events
                        <p></p>
                    </h4>
                </div>

                <div class="row">

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="" title="TBA"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Causal Inference in Python</h3>
                                <p>
                                    Ingolstadt Innovation Conference (TBC)
                                </p>
                            </div>
                        </div>
                    </div>

                </div>

                <div>
                    <h4>
                        Selected past events
                        <p></p>
                    </h4>
                </div>

                <div class="row">

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://2022.pycon.de/" title="Register"><i class="fas fa-code"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Practical graph neural networks in Python with TensorFlow and Spektral (Workshop)</h3>
                                <p>
                                    PyConDE & PyData Berlin 2022 (Apr 13, 2022)
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://www.meetup.com/PyData-Hamburg/events/284616434/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Causality: An Introduction</h3>
                                <p>
                                    PyData Hamburg (Mar 29, 2022)
                                </p>
                                <p>
                                    <a href="https://www.youtube.com/watch?v=kyRUDTexwGM"><i class="fab fa-youtube" style="color: red;"></i></a>
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://ghostday.pl/#agenda" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Causal Disovery in Python</h3>
                                <p>
                                    GHOST Day Applied ML Conf 2022 (Mar 24, 2022)
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://dssconf.pl/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Causal Inference in Python: An Introduction</h3>
                                <p>
                                    Data Science Summit (Dec 3, 2021)
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://www.meetup.com/PyData-Tel-Aviv/events/281354553/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>
                                    What should I buy next? How to leverage word embeddings to build an efficient recommender system
                                </h3>
                                    
                                <p>
                                    PyData Tel Aviv 2021 (Nov 10, 2021)
                                </p>
                                <p>
                                    <a href="https://www.youtube.com/watch?v=_IZUmV28JpA"><i class="fab fa-youtube" style="color: red;"></i></a>
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.0s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://pydata.org/global2021/schedule/presentation/13/modeling-aleatoric-and-epistemic-uncertainty-using-tensorflow-and-tensorflow-probability/" title="Register"><i class="far fa-comment"></i></a>
                            </div>
                            <div class="service-text">
                                <h3>Modeling aleatoric and epistemic uncertainty using Tensorflow and Tensorflow Probability</h3>
                                <p>
                                    PyData Global 2021 (Oct 30, 2021)
                                </p>
                                <p>
                                    <a href="https://www.youtube.com/watch?v=KJxmC5GCWe4"><i class="fab fa-youtube" style="color: red;"></i></a>
                                    
                                </p>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6 wow fadeInUp" data-wow-delay="0.2s">
                        <div class="service-item">
                            <div class="service-icon">
                                <a href="https://www.nlpday.pl/" title="Register"><i class="fas fa-code"></i></a>
                            </div>
                            
                            <div class="service-text">
                                <h3>
                                    Uncertainty? Hands-on Bayesian neural networks with Tensorflow and Tensorflow Probability (Workshop)
                                </h3>
                                <p>
                                    NLP & AI Day 2021 (Oct 26, 2021)
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                    <!-- <div>
                        <h4>
                            Archive
                            <p>
                            </p>
                        </h4>
                    </div>

                <div>
                    <ul>

                        <li>
                            <b>What should I buy next? How to leverage word embeddings to build an efficient recommender system.</b>
                            <br>
                            <a href="https://ml.dssconf.pl/">Data Science Summit ML Edition 2021</a>
                            <br><br>
                        </li>

                        <li>
                            <b>Attention! How to efficiently mask sensitive information in unstructured text data?</b>
                            <br>
                            <a href="https://ml.dssconf.pl/">Data Science Summit ML Edition 2021</a>
                            <br><br>
                        </li>

                        <li>
                            <b>Can I do it for you? A case study of the intelligent data imputation system in Master Data Management</b>
                            <br>
                            <a href="https://www.stibosystems.com/connect2020-welcome/">Stibo Global Connect 2020</a>
                            <br><br>                            
                        </li>

                    </ul>

                </div> -->
            </div>
        </div>
        <!-- Service End -->


        <!-- Banner Start -->
        <div class="banner wow zoomIn" data-wow-delay="0.1s">
            <div class="container">
                <div class="section-header text-center">
                    <!-- <p>Blogging</p> -->
                    <h2>Blogging</h2>
                </div>
                <div class="container banner-text">
                    <a class="btn" href="https://www.linkedin.com/in/aleksandermolak/"><i class="fab fa-linkedin-in"></i> LinkedIn</a>
                    <a class="btn" href="https://aleksander-molak.medium.com/"><i class="fab fa-medium"></i> Medium</a>
                </div>
            </div>
        </div>
        <!-- Banner End -->


        <!-- Medium Blog Start -->
        <div class="blog" id="blogblog">
            <div class="container">
                <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
                    <p>Medium Blog</p>
                    <h2>Articles</h2>
                </div>
                <div class="row">

                    <!-- Posts start here L -> R -->

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".15s">
                            <div class="blog-img">
                                <img src="img/medium-books-Q1-2022.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Three amazing data science books to read in 2023 (if you won’t manage in 2022)</h2>
                                <p>
                                    …and you can read them for free if you want! ❤️
                                </p>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Probabilistic modeling, graph neural networks</p>
                                    <p><i class="far fa-calendar-alt"></i>04-Feb-2022</p>
                                </div>

                                <p>
                                    Kevin Murphy's <a href="https://www.amazon.com/Probabilistic-Machine-Learning-Introduction-Computation/dp/0262046822/ref=sr_1_fkmr1_1?crid=26OWT9WS9ZNIK&amp;keywords=probabilistic+modeling+an+introduction+murphy&amp;qid=1647714837&amp;sprefix=probabilistic+modeling+an+introduction+murphy%252Caps%252C223&amp;sr=8-1-fkmr1&_encoding=UTF8&tag=alxndrmlk0a-20&linkCode=ur2&linkId=d77722e16e8e4f7aef2ab2e20ccb653b&camp=1789&creative=9325">Probabilistic Machine Learning: An Introduction</a>,
                                    <a href="https://www.amazon.com/Bayesian-Modeling-Computation-Chapman-Statistical/dp/036789436X/ref=sr_1_1?crid=20SPWJZNJK0K1&amp;keywords=Bayesian+Modeling+and+Computation+in+Python&amp;qid=1647715035&amp;sprefix=bayesian+modeling+and+computation+in+python%252Caps%252C220&amp;sr=8-1&_encoding=UTF8&tag=alxndrmlk0a-20&linkCode=ur2&linkId=942ad4bca850888e3fd3ed1e069be014&camp=1789&creative=9325">Bayesian Modeling and Computation in Python</a> by Osvaldo Martin and colleagues and 
                                    <a href="https://www.amazon.com/Deep-Learning-Graphs-Yao-Ma/dp/1108831745/ref=sr_1_3?crid=123RUAZEP2CJ6&amp;keywords=3.+Deep+Learning+on+Graphs&amp;qid=1647715188&amp;sprefix=3.+deep+learning+on+graphs%252Caps%252C503&amp;sr=8-3&_encoding=UTF8&tag=alxndrmlk0a-20&linkCode=ur2&linkId=bd8e2dd6260b1fbaa402868fc398cf35&camp=1789&creative=9325">Deep Learning on Graphs</a>. What are they about and what to expect inside? 
                                </p>
                                <a class="btn" href="https://aleksander-molak.medium.com/three-amazing-data-science-books-to-read-in-2023-if-you-wont-manage-in-2022-f35827cbbb1d">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".15s">
                            <div class="blog-img">
                                <img src="img/medium-tfp-04.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Modeling uncertainty in neural networks with TensorFlow Probability - Part 4</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Probabilistic modeling, Bayesian neural networks, TensorFlow Probability</p>
                                    <p><i class="far fa-calendar-alt"></i>26-Nov-2021</p>
                                </div>
                                <p>
                                    Part 4: Going fully probabilistic
                                </p>
                                <a class="btn" href="https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-391b29538a7a">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".15s">
                            <div class="blog-img">
                                <img src="img/medium-tfp-03.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Modeling uncertainty in neural networks with TensorFlow Probability - Part 3</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Probabilistic modeling, Bayesian neural networks, TensorFlow Probability</p>
                                    <p><i class="far fa-calendar-alt"></i>19-Nov-2021</p>
                                </div>
                                <p>
                                    Part 3: Epistemic uncertainty
                                </p>
                                <a class="btn" href="https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-d519a4426e9c">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

        
                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".15s">
                            <div class="blog-img">
                                <img src="img/medium-tfp-02.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Modeling uncertainty in neural networks with TensorFlow Probability - Part 2</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Probabilistic modeling, Bayesian neural networks, TensorFlow Probability</p>
                                    <p><i class="far fa-calendar-alt"></i>12-Nov-2021</p>
                                </div>
                                <p>
                                    Part 2: Aleatoric uncertainty
                                </p>
                                <a class="btn" href="https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-a706c2274d12">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/medium-tfp-01.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Modeling uncertainty in neural networks with TensorFlow Probability - Part 1</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Probabilistic modeling, Bayesian neural networks, TensorFlow Probability</p>
                                    <p><i class="far fa-calendar-alt"></i>03-Nov-2021</p>
                                </div>
                                <p>
                                    Part 1: An Introduction
                                </p>
                                <a class="btn" href="https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-part-1-an-introduction-2bb564c67d6">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                </div>
            </div>
        </div>

        
        
        <!-- Banner Start -->
        <div class="banner wow zoomIn" data-wow-delay="0.1s">
            <div class="container">
                <div class="section-header text-center">
                    <p>Let's stay in touch!</p>
                    <h2>Let's connect!</h2>
                </div>
                <div class="container banner-text">
                    <p>
                        I'd love to invite you to my LinkedIn and Medium networks. Join the community around Sunday AI Papers and connect to thousands of machine learning researchers and practitioners!
                    </p>
                    <a class="btn" href="https://www.linkedin.com/in/aleksandermolak/"><i class="fab fa-linkedin-in"></i></a>
                    <a class="btn" href="https://aleksander-molak.medium.com/"><i class="fab fa-medium"></i></a>
                    <a class="btn" href="https://github.com/AlxndrMlk"><i class="fab fa-github"></i></a>
                </div>
            </div>
        </div>
        <!-- Banner End -->


        <!-- Blog Start -->
        <div class="blog" id="blog">
            <div class="container">
                <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
                    <p>Sunday AI Papers</p>
                    <h2>Latest posts</h2>
                </div>
                <div class="row">

                    <!-- Posts start here L -> R -->

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".1s">
                            <div class="blog-img">
                                <img src="img/blog-26.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>✨ Are 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿𝘀 𝗿𝗼𝗯𝘂𝘀𝘁 to 𝘀𝗽𝘂𝗿𝗶𝗼𝘂𝘀 𝗰𝗼𝗿𝗿𝗲𝗹𝗮𝘁𝗶𝗼𝗻𝘀?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Transformers, spurious correlations, invariance</p>
                                    <p><i class="far fa-calendar-alt"></i>20-Mar-2022</p>
                                </div>
                                <p>
                                    Last Thursday researchers from University of Wisconsin-Madison released a brand new paper 
                                    analyzing the robustness of ViT model to spurious correlations.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6911383218692718592-AwVK?utm_source=linkedin_share&utm_medium=member_desktop_web">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".1s">
                            <div class="blog-img">
                                <img src="img/blog-25.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2> ✨ Scaling 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿𝘀 to thousands of layers? 😯</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, scaling</p>
                                    <p><i class="far fa-calendar-alt"></i>06-Mar-2022</p>
                                </div>
                                <p>
                                    Last Tuesday researchers form Microsoft Research released a new paper introducing 
                                    a new method that allows to build 𝗲𝘅𝘁𝗿𝗲𝗺𝗲𝗹𝘆 𝗱𝗲𝗲𝗽 Transformer models.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_stopwar-sundayaipapers-machinelearning-activity-6906326045860184065-eBeh">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".1s">
                            <div class="blog-img">
                                <img src="img/blog-24.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2> ✨ Making 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿𝘀 4-𝟮𝟭𝘅 𝗳𝗮𝘀𝘁𝗲𝗿, but 𝘄𝗶𝘁𝗵𝗼𝘂𝘁 𝗾𝘂𝗮𝗹𝗶𝘁𝘆 𝗱𝗿𝗼𝗽? Check this! 💥</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, attention</p>
                                    <p><i class="far fa-calendar-alt"></i>27-Feb-2022</p>
                                </div>
                                <p>
                                    Last Monday researchers form Cornell University and Google Brain released 
                                    a new paper presenting 𝗙𝗟𝗔𝗦𝗛 - a novel efficient modification of Transformer 
                                    architecture.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6903793258288549889-C_IR">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".1s">
                            <div class="blog-img">
                                <img src="img/blog-23.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2> Have you ever thought about 𝗰𝗵𝗮𝗻𝗴𝗶𝗻𝗴 your 𝗱𝗮𝘁𝗮 𝗱𝘂𝗿𝗶𝗻𝗴 the 𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴? 😯</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Deep Learning, training, schedules</p>
                                    <p><i class="far fa-calendar-alt"></i>20-Feb-2022</p>
                                </div>
                                <p>
                                    On Thursday, Leslie N. Smith of U.S. Naval Research Laboratory released his new paper on 
                                    general cyclical training. Sound familiar? Maybe, but don't get misled!
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6901232356880187392-SxXx">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".1s">
                            <div class="blog-img">
                                <img src="img/blog-22.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2> 𝗘𝗻𝗱-𝘁𝗼-𝗲𝗻𝗱 Bayesian 𝗰𝗮𝘂𝘀𝗮𝗹 𝗶𝗻𝗳𝗲𝗿𝗲𝗻𝗰𝗲 with theoretical guarantees? Yes! 🤯</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Causality, Bayesian methods</p>
                                    <p><i class="far fa-calendar-alt"></i>13-Feb-2022</p>
                                </div>
                                <p>
                                    On the first Friday of February, researchers from Microsoft Research, University of Cambridge, 
                                    University of Massachusetts Amherst and G-Research released a paper describing a novel method for 
                                    end-to-end causal process.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6898723493614366720-nQ9r">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-21.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Commonsense 𝗰𝗮𝘂𝘀𝗮𝗹𝗶𝘁𝘆 using 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿𝘀?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, causality, Transformers</p>
                                    <p><i class="far fa-calendar-alt"></i>06-Feb-2022</p>
                                </div>
                                <p>
                                    Last Monday, researchers from University of Pennsylvania released a paper proposing a new 
                                    framework to perform 𝗰𝗼𝗺𝗺𝗼𝗻𝘀𝗲𝗻𝘀𝗲 𝗰𝗮𝘂𝘀𝗮𝗹𝗶𝘁𝘆 𝗿𝗲𝗮𝘀𝗼𝗻𝗶𝗻𝗴 (𝗖𝗖𝗥).
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6896191256461201409-EsUK">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>
                    

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-20.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Can 𝗰𝗮𝘂𝘀𝗮𝗹𝗶𝘁𝘆 help in making 𝗰𝗼𝗻𝗱𝗶𝘁𝗶𝗼𝗻𝗮𝗹 𝘁𝗲𝘅𝘁 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝗼𝗻 more 𝘃𝗲𝗿𝘀𝗮𝘁𝗶𝗹𝗲 and 𝗹𝗲𝘀𝘀 𝗯𝗶𝗮𝘀𝗲𝗱?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, causality, VAE</p>
                                    <p><i class="far fa-calendar-alt"></i>30-Jan-2022</p>
                                </div>
                                <p>
                                    Last Saturday, researchers from UC San Diego and Amazon AI released a paper describing a novel approach 
                                    to conditional text generation that leverages causal inference principles to mitigate the effects of spurious correlations. 
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6893652455415238657-PDcO">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-19.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>More 𝘀𝗲𝗺𝗮𝗻𝘁𝗶𝗰𝗮𝗹𝗹𝘆 𝘀𝗲𝗻𝘀𝗶𝘁𝗶𝘃𝗲 contrastive 𝘀𝗲𝗻𝘁𝗲𝗻𝗰𝗲 𝗲𝗺𝗯𝗲𝗱𝗱𝗶𝗻𝗴𝘀?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, contrastive learning</p>
                                    <p><i class="far fa-calendar-alt"></i>23-Jan-2022</p>
                                </div>
                                <p>
                                    Last Wednesday, researchers from The University of Hong Kong, National University of Defense Technology and SenseTime 商汤科技 
                                    released a paper proposing a new contrastive sentence embedding framework called 𝗦𝗡𝗖𝗦𝗘.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6891102099477000196-Vp5J">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-18.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿𝘀 with efficient 𝘂𝗻𝗰𝗲𝗿𝘁𝗮𝗶𝗻𝘁𝘆 𝗲𝘀𝘁𝗶𝗺𝗮𝘁𝗶𝗼𝗻? 😯 Voilà! 🎉</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, uncertainty, Bayesian, attention</p>
                                    <p><i class="far fa-calendar-alt"></i>09-Jan-2022</p>
                                </div>
                                <p>
                                    On the last Monday of December, researchers from University of Amsterdam and Amazon released a paper 
                                    introducing a novel uncertainty estimation method for Transformers. 
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6886040535644037120-mZt2">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-16.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Is causal 𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲 𝗲𝗻𝗰𝗼𝗱𝗲𝗱 in the 𝗹𝗮𝗻𝗴𝘂𝗮𝗴𝗲? Making reinforcement learning agents more robust by asking them to 𝗲𝘅𝗽𝗹𝗮𝗶𝗻 their 𝗱𝗲𝗰𝗶𝘀𝗶𝗼𝗻𝘀.</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>RL, NLP, causality, generalizability</p>
                                    <p><i class="far fa-calendar-alt"></i>12-Dec-2021</p>
                                </div>
                                <p>
                                    Last Wednesday, researchers from DeepMind released a paper describing 
                                    a novel approach to RL-agent training that makes the agents more robust.
                                    Agents were able to learn more generalizable abstractions thanks to... 
                                    explaining their decisions.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ml-activity-6875915126214598656-ezrh">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-15.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>𝗚𝗣𝗨-𝗮𝗰𝗰𝗲𝗹𝗲𝗿𝗮𝘁𝗲𝗱 gradient-based 𝗰𝗮𝘂𝘀𝗮𝗹 𝗱𝗶𝘀𝗰𝗼𝘃𝗲𝗿𝘆 in Python? 𝗛𝗲𝗿𝗲 𝘆𝗼𝘂 𝗴𝗼!</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Causality, causal discovery</p>
                                    <p><i class="far fa-calendar-alt"></i>05-Dec-2021</p>
                                </div>
                                <p>
                                    Last Tuesday, researchers from Huawei Noah's Ark Lab and University of Toronto 
                                    released a new causal discovery package and an accompanying paper. 
                                    It brings some really cool features to the table!
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-causality-activity-6873357904792104960--AHC">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-14.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Can one learn a 𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲 of a 𝗰𝗮𝘂𝘀𝗮𝗹 𝗴𝗿𝗮𝗽𝗵 with 𝗹𝗮𝘁𝗲𝗻𝘁 𝘃𝗮𝗿𝗶𝗮𝗯𝗹𝗲𝘀?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Causality, causal discovery</p>
                                    <p><i class="far fa-calendar-alt"></i>28-Nov-2021</p>
                                </div>
                                <p>
                                    Last Sunday, researchers from University of Chicago and Carnegie Mellon University 
                                    released a paper proposing a novel method of discovering a causal graph with latent 
                                    variables, but the problem is hard.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-causality-activity-6870844568338804736-QvWR">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-13.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>𝗗𝗶𝘀𝘁𝗶𝗹𝗹𝗶𝗻𝗴 𝗕𝗘𝗥𝗧 better? Get 𝗳𝗮𝘀𝘁𝗲𝗿 and more 𝗿𝗼𝗯𝘂𝘀𝘁!</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, distillation, compression</p>
                                    <p><i class="far fa-calendar-alt"></i>21-Nov-2021</p>
                                </div>
                                <p>
                                    Last Thursday, researchers from Intel Labs and University of California, Santa Barbara 
                                    proposed a new approach to model distillation. 
                                    The proposed architecture achieves very good trade-off between 𝗶𝗻𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝘁𝗶𝗺𝗲 𝗿𝗲𝗱𝘂𝗰𝘁𝗶𝗼𝗻 and 𝗮𝗰𝗰𝘂𝗿𝗮𝗰𝘆.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-nlp-activity-6868300191968100352-BVP9">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-12.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Contrastive loss 𝗕𝗘𝗥𝗧 pre-training for 𝗶𝗺𝗽𝗿𝗼𝘃𝗲𝗱 𝗿𝗲𝗽𝗿𝗲𝘀𝗲𝗻𝗮𝘁𝗶𝗼𝗻𝘀? Yes, please!</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, contrastive loss, pre-training</p>
                                    <p><i class="far fa-calendar-alt"></i>14-Nov-2021</p>
                                </div>
                                <p>
                                    Last Tuesday, researchers from University of Cambridge, Amazon Web Services (AWS) AI and 
                                    Monash University released a paper introducing a new pre-training approach leveraging 
                                    contrastive loss scheme. 
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-nlp-activity-6865758937896022016-mDLY">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>
                    
                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-11.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Ok, computer, 𝗽𝗿𝗲𝗽𝗿𝗼𝗰𝗲𝘀𝘀 my 𝘀𝘁𝗿𝗶𝗻𝗴𝘀! 🤖</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, strings, preprocessing</p>
                                    <p><i class="far fa-calendar-alt"></i>07-Nov-2021</p>
                                </div>
                                <p>
                                    Last Thursday researchers from Eindhoven University of Technology released a paper describing a 
                                    framework for automated string preprocessing and encoding. 
                                    The framework leverages probabilistic type inference among other interesting components.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-sundayaipapers-machinelearning-activity-6863146269716598784-v7Hr">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <!-- <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-10.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Is 𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗼𝗯𝘀𝗼𝗹𝗲𝘁𝗲? Can we 𝗽𝗿𝗲𝗱𝗶𝗰𝘁 the 𝘄𝗲𝗶𝗴𝗵𝘁𝘀 instead?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>Parameter search, optimization, GNNs</p>
                                    <p><i class="far fa-calendar-alt"></i>31-Oct-2021</p>
                                </div>
                                <p>
                                    Last Monday, researchers from University of Guelph and Facebook AI 
                                    released a paper presenting a novel framework that predicts parameters 
                                    for an arbitrary network**. What's behind?
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-machinelearning-ai-activity-6860681839154135040-lnFc">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-9.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2> Can you broaden my perspective? 𝗖𝗼𝗻𝘁𝗿𝗮𝘀𝘁𝗶𝘃𝗲 Document Representation Learning with 𝗚𝗿𝗮𝗽𝗵 𝗔𝘁𝘁𝗲𝗻𝘁𝗶𝗼𝗻 𝗡𝗲𝘁𝘄𝗼𝗿𝗸𝘀 over 𝗽𝗿𝗲-𝘁𝗿𝗮𝗶𝗻𝗲𝗱 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿𝘀.</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, contrastive learning, GNNs</p>
                                    <p><i class="far fa-calendar-alt"></i>24-Oct-2021</p>
                                </div>
                                <p>
                                    Last Wednesday, researchers from AWS AI Labs released a paper presenting 
                                    a novel method for 𝗱𝗼𝗰𝘂𝗺𝗲𝗻𝘁 𝗿𝗲𝗽𝗿𝗲𝘀𝗲𝗻𝘁𝗮𝘁𝗶𝗼𝗻. The method uses a large language model, 
                                    a graph attention neural network and a contrastive training strategy. 
                                    Obtained document representations seem to have 𝘃𝗲𝗿𝘆 𝗴𝗼𝗼𝗱 𝗽𝗿𝗼𝗽𝗲𝗿𝘁𝗶𝗲𝘀.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6858125702483496960-Tdh9">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-8.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Hey 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿𝘀, how 𝘀𝗲𝗻𝘀𝗶𝘁𝗶𝘃𝗲 are you to 𝘀𝗽𝘂𝗿𝗶𝗼𝘂𝘀 perturbations?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, causality</p>
                                    <p><i class="far fa-calendar-alt"></i>17-Oct-2021</p>
                                </div>
                                <p>
                                    The ties between NLP and Causal Inference grow stronger. 
                                    Last Thursday, a group of researchers from Peking University and 
                                    Normal University of Singapore released a paper analysing four 
                                    NLP architectures' sensitivity to spurious features using Causal Inference tools.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6855600302754713600-Qfq3">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-7">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-7.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Data collection, 𝗰𝗮𝘂𝘀𝗮𝗹𝗶𝘁𝘆 and 𝗡𝗟𝗣?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, causality</p>
                                    <p><i class="far fa-calendar-alt"></i>10-Oct-2021</p>
                                </div>
                                <p>
                                    Last Friday researchers from Max Planck Institute for Intelligent Systems, 
                                    ETH Zürich, University of Cambridge, UCL and 
                                    Indian Institute of Technology, Kharagpur published a paper 
                                    examining a relationship between data collection process and models 
                                    performance on various NLP tasks. 
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6853075833230286848-sn38">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-6.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Does 𝗕𝗘𝗥𝗧 really 𝗱𝗼 what 𝘆𝗼𝘂 𝘁𝗵𝗶𝗻𝗸 it 𝗱𝗼𝗲𝘀?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, BERT, permutation</p>
                                    <p><i class="far fa-calendar-alt"></i>03-Oct-2021</p>
                                </div>
                                <p>
                                    Last week, we've seen how 𝗽𝗼𝗼𝗿𝗹𝘆 BERT might 𝗴𝗲𝗻𝗲𝗿𝗮𝗹𝗶𝘇𝗲 
                                    in named entity recognition (𝗡𝗘𝗥) tasks (https://lnkd.in/e-zfBNg9). 
                                    This week we'll look at a paper released on Tuesday, 
                                    where researchers from Sberbank and Higher School of Economics 
                                    in Moscow systematically examine 𝘀𝘆𝗻𝘁𝗮𝗰𝘁𝗶𝗰 𝗮𝗯𝗶𝗹𝗶𝘁𝗶𝗲𝘀 of the model.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6850491510715146241-TCai">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay=".3s">
                            <div class="blog-img">
                                <img src="img/blog-5.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>𝗛𝗼𝘄 𝗲𝗮𝘀𝗶𝗹𝘆 could you 𝗺𝗶𝘀𝗹𝗲𝗮𝗱 𝗕𝗘𝗥𝗧 in recognizing 𝗻𝗮𝗺𝗲𝗱 𝗲𝗻𝘁𝗶𝘁𝗶𝗲𝘀 (NER)?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, adversarial attacks</p>
                                    <p><i class="far fa-calendar-alt"></i>26-Sep-2021</p>
                                </div>
                                <p>
                                    Last Thursday researchers from Leiden University released a paper 
                                    examining the influence of 𝗮𝗱𝘃𝗲𝗿𝘀𝗮𝗿𝗶𝗮𝗹 𝗮𝘁𝘁𝗮𝗰𝗸𝘀 on 𝗕𝗘𝗥𝗧's capabilities in 
                                    𝗡𝗮𝗺𝗲𝗱 𝗘𝗻𝘁𝗶𝘁𝘆 𝗥𝗲𝗰𝗼𝗴𝗻𝗶𝘁𝗶𝗼𝗻 (NER).
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6847996331672297473-OhLo">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.3s">
                            <div class="blog-img">
                                <img src="img/blog-4.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Can 𝗿𝗮𝗻𝗱𝗼𝗺𝗹𝘆-𝘄𝗲𝗶𝗴𝗵𝘁𝗲𝗱 one-layer 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿 be as good as a 𝗳𝘂𝗹𝗹𝘆 𝘁𝗿𝗮𝗶𝗻𝗲𝗱 model?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, memory efficiency</p>
                                    <p><i class="far fa-calendar-alt"></i>12-Sep-2021</p>
                                </div>
                                <p>
                                    Last Wednesday researchers from Facebook AI and University of California, Berkeley 
                                    released a paper examining the potential of randomly-weighted one-layer 
                                    Transformer for translation and other NLP tasks.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6842896382714548224-HPG-">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>

                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.3s">
                            <div class="blog-img">
                                <img src="img/blog-3.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Who wants an 𝗲𝗳𝗳𝗶𝗰𝗶𝗲𝗻𝘁 𝘁𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿 for 𝗮𝗿𝗯𝗶𝘁𝗿𝗮𝗿𝗶𝗹𝘆 𝗹𝗼𝗻𝗴 𝘀𝗲𝗾𝘂𝗲𝗻𝗰𝗲𝘀? </h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, Transformers, memory efficiency</p>
                                    <p><i class="far fa-calendar-alt"></i>05-Sep-2021</p>
                                </div>
                                <p>
                                    Last Wednesday researchers from University of Lisbon and DeepMind released 
                                    a paper where they proposed an infinite-memory transformer or ∞-𝙛𝙤𝙧𝙢𝙚𝙧. 
                                    The model is able to model arbitrarily long contexts.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6840375144599367680-IACv">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.3s">
                            <div class="blog-img">
                                <img src="img/blog-2.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Have you ever heard of 𝘃𝗮𝗿𝗶𝗮𝘁𝗶𝗼𝗻𝗮𝗹 𝗮𝘂𝘁𝗼𝗲𝗻𝗰𝗼𝗱𝗲𝗿𝘀 (VAEs) for 𝗡𝗟𝗣?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, variational inference</p>
                                    <p><i class="far fa-calendar-alt"></i>29-Aug-2021</p>
                                </div>
                                <p>
                                    Last Monday researchers from Universidad Carlos III de Madrid and Swiss Data Science Institute (ETHZ/EPFL) 
                                    released a paper, proposing a novel and surprising usage of 𝗩𝗔𝗘s within 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿𝘀 framework. 
                                    𝗡𝗼𝗥𝗕𝗘𝗥𝗧 is a modified 𝗕𝗘𝗥𝗧 architecture with a special VAE component.
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6837843979887808512-jAUO">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div>


                    <div class="col-lg-6">
                        <div class="blog-item wow fadeInUp" data-wow-delay="0.1s">
                            <div class="blog-img">
                                <img src="img/blog-1.jpg" alt="Blog">
                            </div>
                            <div class="blog-text">
                                <h2>Dreaming about 𝗿𝗼𝗯𝘂𝘀𝘁 𝗡𝗘𝗥 models with 𝗽𝗮𝗿𝘁𝗶𝗮𝗹𝗹𝘆 𝗹𝗮𝗯𝗲𝗹𝗹𝗲𝗱 data?</h2>
                                <div class="blog-meta">
                                    <p><i class="far fa-user"></i>Aleksander Molak</p>
                                    <p><i class="far fa-list-alt"></i>NLP, NER, Transformers</p>
                                    <p><i class="far fa-calendar-alt"></i>22-Aug-2021</p>
                                </div>
                                <p>
                                    Last week researchers from Columbia University in the City of New York and 
                                    Google Research released a paper proposing a new approach towards partially 
                                    supervised named entity recognition (NER).
                                </p>
                                <a class="btn" href="https://www.linkedin.com/posts/aleksandermolak_sundayaipapers-nlp-transformers-activity-6835313599288799232-FY-R">Read More <i class="fa fa-angle-right"></i></a>
                            </div>
                        </div>
                    </div> -->




                </div>
            </div>
        </div>
        <!-- Blog End -->


        <!-- Footer Start -->
        <div class="footer wow fadeIn" data-wow-delay="0.3s" id="contact">
            <div class="container-fluid">
                <div class="container">
                    <div class="footer-info">
                        <h2>Let's connect!</h2>
                        <div class="footer-social">
                            <a href="https://www.linkedin.com/in/aleksandermolak"><i class="fab fa-linkedin-in"></i></a>
                            <a href="https://github.com/AlxndrMlk"><i class="fab fa-github"></i></a>
                            <a href="https://twitter.com/AleksanderMolak"><i class="fab fa-twitter"></i></a>
                            <a href="https://www.researchgate.net/profile/Aleksander-Molak"><i class="fab fa-researchgate"></i></a>
                            <a href="https://aleksander-molak.medium.com/"><i class="fab fa-medium"></i></a>
                            <!-- <a href=""><i class="fab fa-youtube"></i></a>
                            <a href=""><i class="fab fa-instagram"></i></a> -->
                        </div>
                    </div>
                </div>
                <div class="container copyright">
                    <p>&copy; <a href="#">alxndr.io 2021</a></p>
                    <p class='small-letters'>Based on a template from <a href="https://htmlcodex.com">HTML Codex</a></p>
                </div>
            </div>
        </div>
        <!-- Footer End -->
        
        
        <!-- Back to top button -->
        <a href="#" class="btn back-to-top"><i class="fa fa-chevron-up"></i></a>
        
        
        <!-- Pre Loader -->
        <div id="loader" class="show">
            <div class="loader"></div>
        </div>

        
        <!-- JavaScript Libraries -->
        <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>
        <script src="lib/easing/easing.min.js"></script>
        <script src="lib/wow/wow.min.js"></script>
        <script src="lib/waypoints/waypoints.min.js"></script>
        <script src="lib/typed/typed.min.js"></script>
        <script src="lib/owlcarousel/owl.carousel.min.js"></script>
        <script src="lib/isotope/isotope.pkgd.min.js"></script>
        <script src="lib/lightbox/js/lightbox.min.js"></script>
        
        <!-- Contact Javascript File -->
        <script src="mail/jqBootstrapValidation.min.js"></script>
        <script src="mail/contact.js"></script>

        <!-- Template Javascript -->
        <script src="js/main.js"></script>
    </body>
</html>
